{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Neural Network Reconstruction of Drought in US\n",
    "# Neural Networks with Sensitivity\n",
    "\n",
    "This code can be used to have tree rings reconstruct a climate index using Neural Networks and compare to a linear method used by Cook et al (1999). In difference to the linear method, here, the closest $n$ stands can be used rather than those at a certain distance. \n",
    "\n",
    "In difference to the Cook et al methodology, this will use the 2.5x2.5 degree scPDSI grid from https://www.esrl.noaa.gov/psd/data/gridded/data.pdsi.html\n",
    "\n",
    "#### AUTHOR: Aleyda M Trevino\n",
    "\n",
    "#### EMAIL: atrevino@g.harvard.edu OR aleydatrevinot@gmail.com\n",
    "\n",
    "#### DATE OF LAST UPDATE: Apr 27, 2021 \n",
    "\n",
    "Things might change to make the code more efficient, easier to read, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT, ARE YOU TESTING OR CHANGING THINGS INSIDE? IF SO MAKE SURE TO PUT 1, \n",
    "# AND SAVE CURRENT VERSION AS THE DATE VERSION\n",
    "TestingCode = 0\n",
    "\n",
    "# For other saving parameters\n",
    "NN_SaveModels = 0\n",
    "NN_SaveResults = 1\n",
    "NN_SaveBasicResults = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are importing parameters\n",
      "You are testing the code\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PACKAGES NEEDED\n",
    "\n",
    "# NUMPY ET AL\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from numpy.fft import rfft\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import loglog\n",
    "import matplotlib.ticker as mticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
    "import mpl_toolkits.axes_grid1.axes_size as Size\n",
    "\n",
    "\n",
    "# SCIPY\n",
    "import scipy\n",
    "import scipy.io as scio\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "from scipy import signal, stats\n",
    "\n",
    "# OTHER MATHS\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics, linear_model\n",
    "\n",
    "# READING DATA FILES\n",
    "import h5py\n",
    "import netCDF4 as nc4\n",
    "\n",
    "# SAVING TIME DETAILS\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "# For mapping/is_land\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import shapely.geometry as sgeom\n",
    "from shapely.ops import unary_union\n",
    "from shapely.prepared import prep\n",
    "\n",
    "# For NN\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LeakyReLU, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow import set_random_seed\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "# ACCESS OS\n",
    "import os\n",
    "\n",
    "# Used for saving things\n",
    "StartTime = time.time()\n",
    "StartStat = copy.deepcopy(dt.datetime.now())\n",
    "\n",
    "# YOU ARE IMPORTING PARAMETERS IF FOLLOWING IS 1\n",
    "ImportParams = 1\n",
    "if ImportParams == 1:\n",
    "    print('You are importing parameters')\n",
    "if TestingCode == 0:\n",
    "    \n",
    "    # For the purposes of running different parameters (making different directories in order to save all imported parameters) \n",
    "    OdysseyIndex = int(copy.deepcopy(os.getcwd().split('-')[-1]))\n",
    "    \n",
    "    # This is the directory where results will be saved. Change this appropriately\n",
    "    SaveResultsFolder = './Results/Test-210322-2/'\n",
    "else:\n",
    "    print('You are testing the code')\n",
    "    OdysseyIndex = 1\n",
    "    \n",
    "    # This is the directory where _Testing_ results will be saved (if you are using this without a bash script, for example)\n",
    "    SaveResultsFolder = './Results/Python/NNTrees/Testing/'\n",
    "    \n",
    "    # If using Jupyter Notebook, this will expand the coding window to fill the internet window you are using. \n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "    \n",
    "OdysseyTest = str(1)\n",
    "\n",
    "# Where are the import parameters being downloaded from: \n",
    "DownloadLocation = './Data/InputSeries_210322-1553.mat'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file will record the actual code running so you can close the file adn have it run in the bbackground\n",
    "csv_status_file = []\n",
    "\n",
    "# This line will say \"DONE\" when all of the code has finished running\n",
    "csv_status_file.append('')\n",
    "\n",
    "# Official starting\n",
    "if TestingCode == 1:\n",
    "    csv_status_file.append('THIS IS USING THE TESTING FILE -- IF NOT INTENTIOANL QUIT NOW')\n",
    "csv_status_file.append('STARTING SERIES CODE ON ODYSSEY')\n",
    "\n",
    "csv_results_file = []\n",
    "csv_results_file.append('Lay,L2Reg,Dropout,PropTrain,DOFUsed,RandomSeed,'+\n",
    "                        'NNCEBetter,NNRv2Better,NNREBetter,NNRc2Better,'+\n",
    "                        'NNCESkill,MLRCESkill,'+\n",
    "                       'NNCEMean,NNREMean,NNRv2Mean,NNRc2Mean,'+\n",
    "                       'MLRCEMean,MLRREMean,MLRRv2Mean,MLRRc2Mean')\n",
    "\n",
    "\n",
    "\n",
    "SecStart = time.time()\n",
    "SecName = \"Functions that will be used\"\n",
    "csv_status_file.append(SecName)\n",
    "if ImportParams == 1 and TestingCode == 0 :\n",
    "    txt_file_name = (SaveResultsFolder+'StatusSeriesFile_'+\n",
    "                     StartStat.strftime(\"%Y%m%d-%H%M\")[2:]+'_Run-'+str(OdysseyIndex)+'_Test-'+OdysseyTest+'.txt')\n",
    "    results_file_name = (SaveResultsFolder+'ResultsSeriesFile_'+\n",
    "                     StartStat.strftime(\"%Y%m%d-%H%M\")[2:]+'_Run-'+str(OdysseyIndex)+'_Test-'+OdysseyTest+'.csv')\n",
    "else:\n",
    "    txt_file_name = (SaveResultsFolder+'StatusSeriesFile_'+\n",
    "                     StartStat.strftime(\"%Y%m%d-%H%M\")[2:]+'_Test-'+OdysseyTest+'.txt')\n",
    "    results_file_name = (SaveResultsFolder+'ResultsSeriesFile_'+\n",
    "                     StartStat.strftime(\"%Y%m%d-%H%M\")[2:]+'_Test-'+OdysseyTest+'.csv')\n",
    "    \n",
    "    \n",
    "with open(txt_file_name, 'w') as f:\n",
    "    for item in csv_status_file:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open(results_file_name, 'w') as g:\n",
    "    for item in csv_results_file:\n",
    "        g.write(\"%s\\n\" % item)\n",
    "\n",
    "        \n",
    "        \n",
    "# FUNCTIONS THAT WILL BE USED\n",
    "# Figure specification\n",
    "plt.rcParams['figure.figsize'] = [25,25]\n",
    "\n",
    "# For determining whether a point is on land or water\n",
    "land_shp_fname = shpreader.natural_earth(resolution='50m',\n",
    "                                       category='physical', name='land')\n",
    "land_geom = unary_union(list(shpreader.Reader(land_shp_fname).geometries()))\n",
    "land = prep(land_geom)\n",
    "\n",
    "def is_land(x, y):\n",
    "    if x>180:\n",
    "        x = x-360\n",
    "    return land.contains(sgeom.Point(x, y))\n",
    "\n",
    "\n",
    "# Finding the nearby stands to a site\n",
    "def Distance_Earth(lat1, long1, lat2, long2):\n",
    "    REarth = 6371\n",
    "\n",
    "    degrees_to_radians = np.pi/180.0\n",
    "\n",
    "    phi1 = np.multiply(90.0*np.ones(np.shape(lat1)) - lat1,np.ones(np.shape(lat1))*degrees_to_radians)\n",
    "    phi2 = np.multiply(90.0*np.ones(np.shape(lat1)) - lat2,np.ones(np.shape(lat1))*degrees_to_radians)\n",
    "\n",
    "    theta1 = long1*degrees_to_radians\n",
    "    theta2 = long2*degrees_to_radians\n",
    "\n",
    "    cos = (np.multiply(np.multiply(np.sin(phi1),np.sin(phi2)),np.cos(theta1 - theta2)) + \n",
    "           np.multiply(np.cos(phi1),np.cos(phi2)))\n",
    "    arc = np.arccos( cos )\n",
    "    dist = arc*REarth\n",
    "    return dist\n",
    "\n",
    "# For the AR processes used\n",
    "class AutoRegression(object):\n",
    "    '''\n",
    "    AR '''\n",
    "    def __init__(self, TimeSeries1, pMin=1, pMax=12,AICcOrBIC='AIC'):\n",
    "        self.TimeSeries = TimeSeries1[:]\n",
    "        self.RawTimeSeries = TimeSeries1[:]\n",
    "        self.pMin = pMin\n",
    "        self.pMax = pMax\n",
    "        self.AICcOrBIC = AICcOrBIC\n",
    "        \n",
    "        RawTimeSeries = TimeSeries1[:]\n",
    "        TimeSeries = self.TimeSeries\n",
    "        pMin = self.pMin\n",
    "        pMax = self.pMax\n",
    "        AICcOrBIC = self.AICcOrBIC\n",
    "        \n",
    "        def regressAR(x1,y1):\n",
    "            from sklearn import linear_model\n",
    "            regr = linear_model.LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)\n",
    "            regr.fit(x1,y1)\n",
    "            YReg = regr.predict(x1).reshape(-1,1)\n",
    "            BetaCoef = regr.coef_.reshape(-1,1)\n",
    "            return YReg, BetaCoef        \n",
    "        \n",
    "        IsNanTimeSeries = np.isnan(TimeSeries)\n",
    "        RawTimeSeries = TimeSeries\n",
    "        TimeSeries = TimeSeries[~IsNanTimeSeries].reshape(-1,1)\n",
    "        self.ARN = len(TimeSeries)\n",
    "        ARN = self.ARN\n",
    "        \n",
    "        if AICcOrBIC == 'AIC':\n",
    "            pMaxAct = np.nanmin((ARN-3,pMax))\n",
    "        else:\n",
    "            pMaxAct = pMax\n",
    "                \n",
    "        \n",
    "        M1 = np.zeros((ARN,ARN))\n",
    "        Phi = np.zeros((pMax,pMax))\n",
    "        Epsilon = np.zeros((ARN,pMax))\n",
    "        Sig = np.zeros((1,pMax))\n",
    "        AICc = np.zeros((1,pMax))\n",
    "        AICPoly = np.zeros((1,pMax))\n",
    "        AICLog = np.zeros((1,pMax))\n",
    "        BIC = np.zeros((1,pMax))\n",
    "        \n",
    "        for i in range(ARN):\n",
    "            for j in range(ARN):\n",
    "                if i-j>0:\n",
    "                    M1[i,j] = TimeSeries[i-j-1]\n",
    "                else:\n",
    "                    M1[i,j] = 0\n",
    "                    \n",
    "        for p in range(pMin,pMaxAct+1):\n",
    "            InterPhi = []\n",
    "            InterReg = []\n",
    "            InterInt = []\n",
    "            InterReg, InterPhi = regressAR(M1[:,0:p],TimeSeries) #, InterInt \n",
    "            Phi[0:p,p-1] = InterPhi.reshape(-1)\n",
    "            Epsilon[0:,p-1] = TimeSeries.reshape(-1,1).reshape(-1) - InterReg.reshape(-1,1).reshape(-1)\n",
    "            Sig[0,p-1] = np.nanstd(Epsilon[0:,p-1])\n",
    "        \n",
    "            AICc[0,p-1] = ARN*np.log(Sig[0,p-1]**2)+2*(p+1)+2*(p+1)*(p+2)/(ARN-p-2)\n",
    "            AICPoly[0,p-1] = 2*(p+1)+2*(p+1)*(p+2)/(ARN-p-2)\n",
    "            AICLog[0,p-1] = ARN*np.log(Sig[0,p-1]**2)\n",
    "            BIC[0,p-1] = (ARN)*np.log(Sig[0,p-1]**2)+p*np.log(ARN)\n",
    "\n",
    "        self.AICc = AICc\n",
    "        self.AICPoly = AICPoly\n",
    "        self.AICLog = AICLog\n",
    "        self.M1 = M1\n",
    "        self.BIC = BIC\n",
    "        self.Sig = Sig\n",
    "        self.Epsilon = Epsilon\n",
    "\n",
    "        if AICcOrBIC == 'AIC':\n",
    "            self.ARP = np.argmin(AICc[0,pMin-1:pMaxAct-1])+ pMin\n",
    "        else:\n",
    "            self.ARP = np.argmin(BIC[0,pMin-1:pMaxAct-1])+ pMin\n",
    "        \n",
    "        InterPhi = []\n",
    "        InterReg = []\n",
    "        InterInt = []\n",
    "        InterReg, InterPhi = regressAR(M1[:,0:self.ARP],TimeSeries) #, InterInt\n",
    "        ARTimeSeries = TimeSeries - InterReg\n",
    "        self.ARCoefs = InterPhi\n",
    "        self.ARInter = InterInt\n",
    "        \n",
    "        self.check = np.zeros(np.shape(TimeSeries))\n",
    "        self.check[0,0] = ARTimeSeries[0,0]\n",
    "\n",
    "        for i in range(1,self.ARN): \n",
    "            self.check[i,0] = (ARTimeSeries[i,0] + \n",
    "                               np.dot(self.check[np.nanmax((i-self.ARP,0)):i,0],\n",
    "                                      np.flip(InterPhi.reshape(-1,1)[0:np.nanmin((self.ARP,i)),\n",
    "                                                                     0],0)).reshape(-1,1))\n",
    "        \n",
    "        if np.sum(np.abs(self.check - TimeSeries))>.01:\n",
    "            print('Error found')\n",
    "            self.Error = 1\n",
    "        \n",
    "        self.ARTimeSeries = np.nan*np.ones(np.shape(RawTimeSeries.reshape(-1,1)))\n",
    "        self.ARTimeSeries[~IsNanTimeSeries.reshape(-1),0] = ARTimeSeries.reshape(-1)\n",
    "        self.TimeSeries = self.RawTimeSeries[:]\n",
    "\n",
    "# Undoing the AR process at the end\n",
    "def InvAR(ARTimeSeries,PhiCoefs):\n",
    "    TimeSeries = np.zeros(np.shape(ARTimeSeries.reshape(-1,1)))\n",
    "    TimeSeries[0,0] = ARTimeSeries[0,0]\n",
    "    ARN = np.max(np.shape(ARTimeSeries))\n",
    "    ARP = np.max(np.shape(PhiCoefs))\n",
    "\n",
    "    for i in range(1,ARN): \n",
    "        TimeSeries[i,0] = (ARTimeSeries[i,0] +\n",
    "                      np.dot(TimeSeries[np.nanmax((i-ARP,0)):i,0],\n",
    "                             np.flip(PhiCoefs.reshape(-1,1)[0:np.nanmin((ARP,i)),0],0)).reshape(-1,1))\n",
    "    return TimeSeries\n",
    "\n",
    "\n",
    "def RegressNoInt(x1,y1):\n",
    "    \n",
    "    regr = linear_model.LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)\n",
    "    regr.fit(x1,y1)\n",
    "    YReg = regr.predict(x1).reshape(-1,1)\n",
    "    BetaCoef = regr.coef_.reshape(-1,1)\n",
    "    return YReg, BetaCoef \n",
    "\n",
    "# finding the significantly correlated stands to a site's climate\n",
    "def SigCorrel(DoF, Alpha):\n",
    "    Co = np.linspace(0,1,10000)\n",
    "    Tx = np.linspace(0,100,100000)\n",
    "    Tcdf = stats.t.cdf(Tx,DoF)\n",
    "    TSig = Tx[np.argmin(np.abs(Tcdf-(1-Alpha/2)))]\n",
    "    CoFun = (np.sqrt(DoF-2)*np.multiply(Co,1./(np.sqrt(np.ones(np.shape(Co))-np.multiply(Co,Co)))))\n",
    "    CoSig = Co[np.argmin(np.abs(CoFun-TSig))]\n",
    "    return CoSig\n",
    "\n",
    "# defining n of AR(n)\n",
    "def RegressAICc(x1,y1):\n",
    "    Resid = np.zeros(np.shape(x1))\n",
    "    Resid.fill(np.nan)\n",
    "    ResidStDev = np.zeros(np.shape(x1)[1])\n",
    "    ResidStDev.fill(np.nan)\n",
    "    AICc = np.zeros(np.shape(x1)[1])\n",
    "    AICc.fill(np.nan)\n",
    "    AICcN = np.shape(x1)[0]\n",
    "    for i in range(np.min([np.shape(x1)[1],AICcN-3])):\n",
    "        p=i*1.0\n",
    "        yTry, yCoef = RegressNoInt(x1[:,0:i+1],y1)\n",
    "        Resid[:,i] = (y1.reshape(-1,1)-yTry.reshape(-1,1)).reshape(-1)\n",
    "        ResidStDev[i] = np.std(Resid[:,i])\n",
    "        AICc[i] = AICcN*np.log(ResidStDev[i]**2)+2*(p+1)+2*(p+1)*(p+2)/(AICcN-p-2)\n",
    "    AICcMin = np.argmin(AICc)+1\n",
    "    YReg, BetaCoef = RegressNoInt(x1[:,0:AICcMin],y1)\n",
    "    return YReg, BetaCoef, AICcMin\n",
    "\n",
    "# Example parameters that can be used in the following function\n",
    "#NN_Params = [3,11,'keras.layers.LeakyReLU(alpha=0.3)',1.3,[0,1,1],50]\n",
    "\n",
    "# THIS IS THE FUNCTION THAT DEALS WITH THE NN MECHANICS. \n",
    "def NN_Calc(x_tr,y_tr,x_va,x_to,x_other,Params):\n",
    "    '''\n",
    "    Runs the training and testing on a Random Forest\n",
    "    INPUTS: \n",
    "        Training and testing:\n",
    "            x_tr: x training [yrs train,num]\n",
    "            y_tr: y training [yrs train,1]\n",
    "            x_va: x validate [yrs validate,num]\n",
    "            x_to: x total [yrs total,num]\n",
    "    \n",
    "        Params: \n",
    "            Params[0]\n",
    "            # Test Layers\n",
    "            TestLayers = 2\n",
    "            \n",
    "            Params[1]\n",
    "            # Hidden nodes\n",
    "            HiddenNodes = 11\n",
    "            \n",
    "            Params[2]\n",
    "            # Activation Term\n",
    "            Activation = 'keras.layers.LeakyReLU(alpha=0.3)'\n",
    "\n",
    "            \n",
    "            Params[3]\n",
    "            # L2 Regularization weight\n",
    "            TestRegWeight  = 1.3\n",
    "\n",
    "            Params[4]\n",
    "            # Dropout Term [Input, all, every nth layer incl layer 1]\n",
    "            TestDropout = [0,0,0]\n",
    "            \n",
    "            Params[5]\n",
    "            # Training epochs\n",
    "            TrainEpochs = [0,0,0]\n",
    "            \n",
    "    \n",
    "    OUTPUT:\n",
    "        y_ReconTr: recon training\n",
    "        y_ReconVa: recon testing\n",
    "        y_ReconTo: recon total\n",
    "        NN_Importance: importance of inputs (features)\n",
    "        NN_Sens_Indiv_Mean: sensitivity analysis per stand (other stands mean) [x_used, y_result]\n",
    "        NN_Sens_Indiv_Mean: sensitivity analysis per stand (other stands real)\n",
    "        NN_Sens_Group: sensitivity wrt group\n",
    "        NN_Sens_Group_Same: sensitivity wrt group same input'''\n",
    "\n",
    "    #seed(Params[3])\n",
    "    keras.backend.clear_session()\n",
    "    Model = Sequential()\n",
    "    \n",
    "    NN_UseEarlyStopping = 0\n",
    "    NN_ActivationStatement = 'LeakyReLU'\n",
    "    NN_RegType = 'l2'\n",
    "    \n",
    "    NN_HiddenLayers = Params[0]\n",
    "    NN_HiddenNodes = Params[1]\n",
    "    NN_Activation = Params[2]\n",
    "    NN_RegWeight = Params[3]\n",
    "    NN_DropoutInput = Params[4][0]\n",
    "    NN_Dropout = Params[4][1]\n",
    "    NN_DropoutMod = Params[4][2]\n",
    "    NN_TrainEpochs = Params[5]\n",
    "\n",
    "    # Addind the layers\n",
    "    Model.add(Dropout(NN_DropoutInput))\n",
    "    for j in range(NN_HiddenLayers):\n",
    "\n",
    "        if NN_ActivationStatement == 'LeakyReLU':\n",
    "            eval('Model.add(Dense(units='+str(int(NN_HiddenNodes))+',input_dim=np.shape(x_tr)[1],'+\n",
    "                 'kernel_regularizer=regularizers.'+NN_RegType+'('+str(NN_RegWeight)+')))')\n",
    "            eval('Model.add('+NN_Activation[NN_Activation.find('LeakyReLU'):]+')')    \n",
    "        else:\n",
    "            eval('Model.add(Dense(units='+str(int(NN_HiddenNodes))+',input_dim=np.shape(x_tr)[1],'+\n",
    "                 'activation='+NN_Activation+',kernel_regularizer=regularizers.'+NN_RegType+'('+str(NN_RegWeight)+')))')\n",
    "        \n",
    "        # adding dropout every nth layer\n",
    "        if j%NN_DropoutMod == 0:\n",
    "            Model.add(Dropout(NN_Dropout))\n",
    "\n",
    "    # linear final layer\n",
    "    Model.add(Dense(units=1,activation='linear'))\n",
    "    \n",
    "    # compile and loss function\n",
    "    # can use 'loss=tf.keras.losses.MeanAbsoluteError()' or \"mean_absolute_error\"\n",
    "    Model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # Are you using early stopping (check earlier, usually no)\n",
    "    if NN_UseEarlyStopping == 1:\n",
    "        ModelCallbacks = [EarlyStopping(monitor='val_loss', patience=NN_EarlyStoppingPatience),\n",
    "                             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "        history = Model.fit(x_tr,#train_features, # Features\n",
    "                          y_tr, # Target vector\n",
    "                          epochs=50, # Number of epochs\n",
    "                          callbacks=ModelCallbacks, # Early stopping\n",
    "                          verbose=0, # Print description after each epoch\n",
    "                          batch_size=1, # Number of observations per batch\n",
    "                          validation_data=(x_va, y_va)) # Data for evaluation\n",
    "    else:\n",
    "\n",
    "        Model.fit(x_tr, y_tr, epochs=NN_TrainEpochs, batch_size=1,verbose=0);\n",
    "\n",
    "    \n",
    "    y_ReconVa = Model.predict(x_va).reshape(-1,1)\n",
    "    y_ReconTr = Model.predict(x_tr).reshape(-1,1)\n",
    "    y_ReconTo = Model.predict(x_to).reshape(-1,1)\n",
    "    y_ReconOther = Model.predict(x_other).reshape(-1,1)\n",
    "    #print('\\tDone train, test, total.')\n",
    "    \n",
    "    # Sensitivity of group (change all inputs from min to max concurrently), might have something funky happening as the covariance structure of growth is changing\n",
    "    x_Sens_Group = np.sort(x_to,axis=0)\n",
    "    y_Sens_Group = Model.predict(x_Sens_Group).reshape(-1,1)\n",
    "    NN_Sens_Group = [copy.deepcopy(x_Sens_Group),copy.deepcopy(y_Sens_Group)]\n",
    "    #print('\\tDone RF Sens Group.')\n",
    "    \n",
    "    # Sensitivity of group but all stands are given the same growth\n",
    "    x_Sens_Group_Same = np.tile(np.linspace(np.min(x_to),np.max(x_to),np.shape(x_to)[0]).reshape(-1,1),(1,np.shape(x_to)[1]))\n",
    "    y_Sens_Group_Same = Model.predict(x_Sens_Group_Same).reshape(-1,1)\n",
    "    NN_Sens_Group_Same = [copy.deepcopy(x_Sens_Group_Same),copy.deepcopy(y_Sens_Group_Same)]\n",
    "    \n",
    "    # go one by one input, replace rest with mean.\n",
    "    y_Sens_Indiv = np.zeros(np.shape(x_to))\n",
    "    y_Sens_Indiv.fill(np.nan)\n",
    "    for i in range(np.shape(x_to)[1]):\n",
    "        x_Sens_Indiv = np.tile(np.mean(x_to,0),[np.shape(x_to)[0],1])\n",
    "        x_Sens_Indiv[:,i] = copy.deepcopy(np.sort(x_to[:,i]))\n",
    "        y_Sens_Indiv[:,i] = Model.predict(x_Sens_Indiv).reshape(-1)\n",
    "    NN_Sens_Indiv_Mean = [copy.deepcopy(np.sort(x_to,axis=0)),copy.deepcopy(y_Sens_Indiv)]\n",
    "    #print('\\tDone RF Sens Indiv, all other mean.')\n",
    "    \n",
    "    # change the input of each one separately by perturbation\n",
    "    y_Sens_Indiv = np.zeros(np.shape(x_to))\n",
    "    y_Sens_Indiv.fill(np.nan)    \n",
    "    # too slow: for i in range(np.shape(x_to)[0]): # Time\n",
    "    for j in range(np.shape(x_to)[1]): # proxy num\n",
    "        Inter_Added = np.zeros((np.shape(x_to)))\n",
    "        Inter_Added[:,j] = 1*np.std(x_to[:,j])*np.ones((np.shape(x_to)[0])).reshape(-1)\n",
    "        x_Sens_Indiv =  x_to + Inter_Added\n",
    "        y_Sens_Indiv[:,j] = (Model.predict(x_Sens_Indiv).reshape(-1) - \n",
    "                            Model.predict(x_to).reshape(-1))\n",
    "    NN_Sens_Indiv_Sprt = [x_to,copy.deepcopy(y_Sens_Indiv),y_ReconTo]\n",
    "#     print('\\tDone RF Sens Indiv, all other actual vals.')\n",
    "    \n",
    "    return [Model, #0\n",
    "            y_ReconTr, #1\n",
    "            y_ReconVa, #2 \n",
    "            y_ReconTo, #3\n",
    "            y_ReconOther, #4\n",
    "            NN_Sens_Group, #5\n",
    "            NN_Sens_Indiv_Mean, #6\n",
    "            NN_Sens_Indiv_Sprt, #7\n",
    "            NN_Sens_Group_Same] #8\n",
    "\n",
    "\n",
    "def ChangeDist(Array,NewMean,NewStd):\n",
    "    '''Assumes arrays ate time*sites, the three inputs have the same dimensions'''\n",
    "    ArrayStd = np.multiply((Array - np.tile(np.mean(Array,axis=0).reshape(1,-1),(np.shape(Array)[0],1))),\n",
    "                           1./np.tile(np.std(Array,axis=0).reshape(1,-1),(np.shape(Array)[0],1)))\n",
    "    ArrayNew = np.multiply(ArrayStd,NewStd)+NewMean\n",
    "    \n",
    "    return ArrayNew\n",
    "\n",
    "\n",
    "# As with other times, it will tell the time when this portion has run\n",
    "print(\"%s seconds to run %s\" % (round(time.time() - SecStart,4),SecName))\n",
    "text = (\"%s minutes to run %s\" % (round((time.time() - SecStart)/60,4),SecName))\n",
    "csv_status_file.append(text)\n",
    "with open(txt_file_name, 'w') as f:\n",
    "    for item in csv_status_file:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section deals with the parametes that will be used\n",
    "\n",
    "SecStart = time.time()\n",
    "SecName = \"Parameters\"\n",
    "csv_status_file.append(SecName)\n",
    "with open(txt_file_name, 'w') as f:\n",
    "    for item in csv_status_file:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Parameters\n",
    "NN_Variable = 'scPDSI'\n",
    "\n",
    "# File source for climate \n",
    "NN_ClimFileSou = './Data/pdsi.mon.mean.selfcalibrated.nc'\n",
    "\n",
    "# Stand data \n",
    "\n",
    "TreeFileSource = './Data/For_Aleyda_10_nov_2017.mat'\n",
    "\n",
    "# This geos ahead and uses the stands or tree chronologies most correlated\n",
    "# to the scPDSI over the training period as the inputs for the NN \n",
    "NN_UseSameInputMLR = 0\n",
    "\n",
    "# Are we distribution matching in the end (over the training period)?\n",
    "NN_DoDistributionMatch = 0\n",
    "\n",
    "# Uncertain if only removing mean is enough (seeing as though the activation function is a leaky ReLU)\n",
    "# If the following is 1, the output will be standardized prior to training (PCA and NN)\n",
    "NN_StandardizeBeforeMethod = 1\n",
    "\n",
    "# Are you allowing for multiple stand chronologies from the same site to enter?\n",
    "NN_AllowDuplicateStands = True\n",
    "\n",
    "# Time and Space Constraints\n",
    "# Cook et al (1999)\n",
    "NN_CaliYearMin = 1928\n",
    "NN_CaliYearMax = 1978\n",
    "NN_ValiYearMin = 1890\n",
    "NN_ValiYearMax = 1927\n",
    "\n",
    "# Actual Tree years that will be used\n",
    "NN_TreeYearMin = 1800\n",
    "NN_TreeYearMax = 1980\n",
    "\n",
    "# Lat-Lon limitations of site and trees\n",
    "NN_LatMax = 50\n",
    "NN_LatMin = 25\n",
    "NN_LonMax = -60\n",
    "NN_LonMin = -130\n",
    "\n",
    "# Allowing for stands a bit further away than the constraints of sites\n",
    "NN_LatDeltaTree = 10\n",
    "NN_LonDeltaTree = 10\n",
    "\n",
    "# From the Clim Data (look at climate data for this)\n",
    "NN_ClimYearMin = 1850\n",
    "NN_ClimYearMax = 2014\n",
    "\n",
    "# The actual analysis years\n",
    "NN_ClimYearAnaMin = 1890\n",
    "NN_ClimYearAnaMax = 1978\n",
    "\n",
    "# Months (inclusive) that will be looked at for clim (averaged over these months)\n",
    "NN_ClimMonthMin = 6\n",
    "NN_ClimMonthMax = 8\n",
    "\n",
    "# AR PROCESS\n",
    "NN_ARTimeDelta = 3\n",
    "\n",
    "# Do not want n in AR(n) to be larger than ARPMax\n",
    "NN_ARPMin = 1\n",
    "NN_ARPMax = 12\n",
    "\n",
    "# These are the years that will be looked at for the overlap of instrumental and stands\n",
    "# The AR_TimeDelta is considered later (removing the initial years)\n",
    "NN_YearMin = np.min([NN_CaliYearMin,NN_ValiYearMin])#-NN_ARTimeDelta, considerado en \"Load Climate\"\n",
    "NN_YearMax = np.max([NN_CaliYearMax,NN_ValiYearMax])\n",
    "\n",
    "# The minimum number of stands that will be used. \n",
    "NN_MinTrees = 50\n",
    "\n",
    "# Maximum distance of stands to site, Cook et al (1999) uses 450km for example\n",
    "# if MaxDist == 0, will use the MinTrees number of closest stands\n",
    "NN_MaxDist = 0 #(km) \n",
    "\n",
    "# Actual max distance to look at if MinTrees is used\n",
    "NN_MaxDistConstraint = 450000\n",
    "\n",
    "# IF NOT USING IMPORT PARAMETERS\n",
    "# NEURAL NETWORK\n",
    "\n",
    "# proportion of data (stand-year combos) to use in defining the NN architecture\n",
    "NN_UseAdaptiveNumNodes = 0.5 # proportion of degrees of freedom used (number of HiddenNodes)\n",
    "\n",
    "NN_HiddenLayers = 2\n",
    "\n",
    "# If UseAdaptiveNumNodes == 0, then define the architecture. \n",
    "NN_HiddenNodes = 20\n",
    "\n",
    "# Training epochs\n",
    "NN_TrainEpochs = 50\n",
    "\n",
    "# Activation function of choice\n",
    "NN_Activation = 'keras.layers.LeakyReLU(alpha=0.3)' #.3Make sure this is. a string argument -- using evals statements\n",
    "\n",
    "# Statement regarding the activation function\n",
    "NN_ActivationStatement = 'LeakyReLU'#'ReLu'#'LeakyReLU'\n",
    "\n",
    "# Regularization used?\n",
    "NN_RegType = 'l2'\n",
    "\n",
    "# Weight for regularization\n",
    "NN_RegWeight = 1.3\n",
    "\n",
    "# Increase from 0 to use dropout proportion\n",
    "NN_Dropout = 0.0\n",
    "\n",
    "# If DropoutMod is 1, dropout is applied to every layer, otherwise, every n layers\n",
    "NN_DropoutMod = 1 # before adding, this was 2\n",
    "\n",
    "# Use a Dropout layer in the input layer?\n",
    "NN_DropoutInput = 0\n",
    "\n",
    "# For Sensitivity Analysis (OLD)\n",
    "NN_PctExtremes = 10 # For sensitivity analysis\n",
    "NN_SensFactor = 0.1 #factor that gets multiplied by the stdev of the tree being analyzed for sensitivity\n",
    "\n",
    "# If early stopping is used. \n",
    "NN_UseEarlyStopping = 0\n",
    "NN_EarlyStoppingPatience = 2\n",
    "\n",
    "# Are you using randomized training or testing?, proportion of years that will be used. \n",
    "NN_UseRandTrainTest = 0\n",
    "\n",
    "# Changin the variable names IF NOT IMPORTING PARAMETERS\n",
    "if ImportParams == 0 or TestingCode == 1 :\n",
    "    NN_TestLayers = np.array([NN_HiddenLayers])\n",
    "    NN_TestRegWeight = copy.deepcopy(np.array([NN_RegWeight]))\n",
    "    NN_TestDropout = copy.deepcopy(np.array([NN_Dropout]))\n",
    "    NN_TestRandTimes = (10+np.cumsum(np.ones(1))).astype(int)\n",
    "    NN_TestRandPropTrain = np.array([NN_UseRandTrainTest])#.573\n",
    "    NN_TestDOFPropToUse = copy.deepcopy(np.array([NN_UseAdaptiveNumNodes]))\n",
    "\n",
    "# IF IMPORTING PARAMETERS, THESE ARE USED     \n",
    "else:\n",
    "    # Where the imput parameters are \n",
    "    DataFileSource = DownloadLocation\n",
    "    \n",
    "    # Loading the file\n",
    "    DataFile = scio.loadmat(DataFileSource) \n",
    "    \n",
    "    # Number of laters\n",
    "    NN_TestLayers = DataFile['NN_TestLayers'][OdysseyIndex,:].astype(int)\n",
    "    \n",
    "    # Weights of regularization\n",
    "    NN_TestRegWeight = DataFile['NN_TestRegWeight'][OdysseyIndex,:]\n",
    "    \n",
    "    # Dropout used\n",
    "    NN_TestDropout = DataFile['NN_TestDropout'][OdysseyIndex,:]\n",
    "    \n",
    "    # Random numbers used to initiate the randomized testing/training\n",
    "    NN_TestRandTimes = DataFile['NN_TestRandTimes'][0,:].astype(int)\n",
    "    \n",
    "    # proportion of testing years that will be used. CAREFUL HERE. \n",
    "    NN_TestRandPropTrain = DataFile['NN_TestRandPropTrain'][OdysseyIndex,:]\n",
    "    \n",
    "    # Proportion of stand-year combos that are used to define architecture\n",
    "    NN_TestDOFPropToUse = DataFile['NN_TestDOFProp'][OdysseyIndex,:]\n",
    "\n",
    "    # Loading the description of test\n",
    "    text = DataFile['Description'][0]\n",
    "    \n",
    "    # Writing the file\n",
    "    csv_status_file.append('Loading external data from: '+DownloadLocation)\n",
    "    csv_status_file.append('Saving results in folder: '+SaveResultsFolder)\n",
    "    csv_status_file.append(text)\n",
    "    text = ('Actual Test: '+str(NN_TestLayers)+'\\n RegWeight: '+str(NN_TestRegWeight)+'\\n'+\n",
    "                     ' Dropout: '+str(NN_TestDropout)+'\\n RandTrainProp: '+str(NN_TestRandPropTrain)+'\\n'+\n",
    "                     ' Random Seeds: '+str(NN_TestRandTimes)+'\\n'+\n",
    "                     ' DOF Used: '+str(NN_TestDOFPropToUse))\n",
    "    csv_status_file.append(text)\n",
    "    with open(txt_file_name, 'w') as f:\n",
    "        for item in csv_status_file:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "#Defined by Cook et al 1999 (for the correlation)\n",
    "MLR_AlphaVal = 0.1\n",
    "\n",
    "print(\"%s seconds to run %s\" % (round(time.time() - SecStart,4),SecName))\n",
    "text = (\"%s minutes to run %s\" % (round((time.time() - SecStart)/60,4),SecName))\n",
    "csv_status_file.append(text)\n",
    "with open(txt_file_name, 'w') as f:\n",
    "    for item in csv_status_file:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "# Initiating some variables\n",
    "NN_TestSkillRc2 = []\n",
    "NN_TestSkillRv2 = []\n",
    "NN_TestSkillRE = []\n",
    "NN_TestSkillCE = []\n",
    "NN_TestDataSplit = []\n",
    "NN_TestReconClim = []\n",
    "NN_TestClimClim = []\n",
    "NN_TestRegUsed = []\n",
    "NN_TestDrpUsed = []\n",
    "NN_TestLayUsed = []\n",
    "NN_TestRndUsed = []\n",
    "NN_TestPrpUsed = []\n",
    "NN_TestDOFUsed = []\n",
    "MLR_TestSkillRc2 = []\n",
    "MLR_TestSkillRv2 = []\n",
    "MLR_TestSkillRE = []\n",
    "MLR_TestSkillCE = []\n",
    "MLR_TestReconClim = []\n",
    "\n",
    "# Keeping track of how many times this is running\n",
    "InterCountTestingAll = 0\n",
    "\n",
    "# for every random number initor\n",
    "for iTestRnd in range(len(NN_TestRandTimes)):\n",
    "    \n",
    "    # For every regweight used. \n",
    "    for iTestReg in range(len(NN_TestRegWeight)):\n",
    "        \n",
    "        # Load random number seed\n",
    "        NN_SeedNo = copy.deepcopy(NN_TestRandTimes[iTestRnd])\n",
    "        \n",
    "        # Load these other vars\n",
    "        NN_HiddenLayers = copy.deepcopy(NN_TestLayers[iTestReg])\n",
    "        NN_RegWeight = copy.deepcopy(NN_TestRegWeight[iTestReg])\n",
    "        NN_Dropout = copy.deepcopy(NN_TestDropout[iTestReg])\n",
    "        NN_UseRandTrainTest = copy.deepcopy(NN_TestRandPropTrain[iTestReg])\n",
    "        NN_UseAdaptiveNumNodes = copy.deepcopy(NN_TestDOFPropToUse[iTestReg])\n",
    "        PrelimSeries = np.arange(NN_ClimYearAnaMax-NN_ClimYearAnaMin+1)\n",
    "        \n",
    "        # Careful with the \"train_test_split\" function, possibly cuts off/rounds percentages for test/train to some sigfigs\n",
    "        # If used as a fraction (ie NumYearsTest/TotOverlapYears) the following is useful\n",
    "        NN_UseRandTrainTest = np.ceil(NN_UseRandTrainTest*1000000)/1000000.\n",
    "        \n",
    "        # Track which years are used for what with the random state used. \n",
    "        Prelim_tr, Prelim_va, Prelim_tr, Prelim_va = train_test_split(PrelimSeries,\n",
    "                                                                      PrelimSeries, \n",
    "                                                                      test_size=1-NN_UseRandTrainTest, \n",
    "                                                                      random_state=NN_SeedNo)\n",
    "        \n",
    "        # set seed (in case both of these are used differently )\n",
    "        seed(NN_SeedNo)\n",
    "        set_random_seed(NN_SeedNo)\n",
    "            \n",
    "        \n",
    "        # Loading climate\n",
    "        SecStart = time.time()\n",
    "        SecName = \"Load Data: Climate\"\n",
    "        \n",
    "        \n",
    "        NN_ClimFile = nc4.Dataset(NN_ClimFileSou,'r')\n",
    "        #Format expressed here, in the case of PDSI, it is pdsi(time, lat, lon)\n",
    "        \n",
    "        # If confused as to how variables look, uncomment following\n",
    "        #print(ClimFile.variables)\n",
    "        \n",
    "        # Load the variables\n",
    "        NN_ClimVar = NN_ClimFile.variables['pdsi'][:,:,:]\n",
    "        NN_ClimLat = NN_ClimFile.variables['lat'][:]\n",
    "        NN_ClimLon = NN_ClimFile.variables['lon'][:]\n",
    "        \n",
    "        \n",
    "        # Careful with how the NaNs are defined. \n",
    "        NN_ClimVar[NN_ClimVar <= -9999] = np.nan\n",
    "        \n",
    "        # Making the arrays for the used climate data\n",
    "        NN_ClimLimData = np.zeros((NN_YearMax-NN_YearMin+1+NN_ARTimeDelta,0))\n",
    "        NN_ClimLimLon = np.zeros((0,1))\n",
    "        NN_ClimLimLat = np.zeros((0,1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # A counter\n",
    "        InterCount = 0\n",
    "        \n",
    "        # Making the climate data files \n",
    "        for i in range(len(NN_ClimLat)):\n",
    "            for j in range(len(NN_ClimLon)):\n",
    "                \n",
    "                # checking whether the lat-lon are on land and within the limits of lat and lon\n",
    "                if (is_land(NN_ClimLon[j],NN_ClimLat[i]) and NN_ClimLat[i]>=NN_LatMin and NN_ClimLat[i]<=NN_LatMax and \n",
    "                    NN_ClimLon[j]>=NN_LonMin and NN_ClimLon[j]<=NN_LonMax and \n",
    "                    # checking for no nans over the period of interest (using mask or using nan)\n",
    "                    np.sum(np.isnan(NN_ClimVar[(NN_YearMin-NN_ClimYearMin)*12:(NN_YearMax-NN_ClimYearMin+1)*12,i,j]))==0): \n",
    "                    \n",
    "                    # this assumes that the climate data is in months\n",
    "                    InterClim = np.array(NN_ClimVar[(NN_YearMin-NN_ClimYearMin-NN_ARTimeDelta)*12:(NN_YearMax-NN_ClimYearMin+1)*12,i,j].reshape(-1,12))\n",
    "                    \n",
    "                    # taking the mean over the months of interest. Changed this recently -- used to not account for the indexing in zero \n",
    "                    InterClim = np.nanmean(InterClim[:,NN_ClimMonthMin-1:NN_ClimMonthMax],1) \n",
    "#                     InterClim = np.nanmean(InterClim[:,NN_ClimMonthMin:NN_ClimMonthMax],1) \n",
    "                    \n",
    "                    # Making the data arrays that are used. \n",
    "                    NN_ClimLimData = np.concatenate((NN_ClimLimData,InterClim.reshape(-1,1)),axis=1)\n",
    "                    NN_ClimLimLon = np.concatenate((NN_ClimLimLon,NN_ClimLon[j].reshape(-1,1)),axis=0)\n",
    "                    NN_ClimLimLat = np.concatenate((NN_ClimLimLat,NN_ClimLat[i].reshape(-1,1)),axis=0)\n",
    "                    \n",
    "                    # CHECK HERE\n",
    "                    # Counting number of sites (print if want to know)\n",
    "                    InterCount = InterCount+1\n",
    "\n",
    "        NN_ClimDataToStat =  copy.deepcopy(NN_ClimLimData[NN_ARTimeDelta:,:] )          \n",
    "        print(\"%s seconds to run %s\" % (round(time.time() - SecStart,4),SecName))\n",
    "        text = (\"%s minutes to run %s\" % (round((time.time() - SecStart)/60,4),SecName))\n",
    "        csv_status_file.append(text)\n",
    "        with open(txt_file_name, 'w') as f:\n",
    "            for item in csv_status_file:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "        # Raw data that will be used. \n",
    "        NN_TreeLimData = np.zeros((NN_TreeYearMax-NN_TreeYearMin+2,0))\n",
    "        NN_TreeLimLon = np.zeros((0,1))\n",
    "        NN_TreeLimLat = np.zeros((0,1))\n",
    "        NN_TreeSite = []\n",
    "\n",
    "        \n",
    "        #TreeFile = scio.loadmat(TreeFileSource) #This could not work with MATLAB v7.3 files. \n",
    "        TreeFile = h5py.File(TreeFileSource, 'r')\n",
    "\n",
    "        #print(list(TreeFile.keys()))\n",
    "        TreeARSTAN = TreeFile['M']\n",
    "        \n",
    "        #print(list(TreeARSTAN.keys()))\n",
    "        \n",
    "        # your chon means\n",
    "        # data is in [chon, years]\n",
    "        TreeChron = np.array(TreeARSTAN['chron_mean'])\n",
    "        \n",
    "        # your stand lat and lon\n",
    "        TreeLat = np.array(TreeARSTAN['latitude'][0])\n",
    "        TreeLon = np.array(TreeARSTAN['longitude'][0])\n",
    "        \n",
    "        \n",
    "#         f = h5py.File(TreeFileSource, 'r')\n",
    "        #print f.keys()\n",
    "    \n",
    "        # If you are screening the stands for repeat chrons, will use first one that appears, load their sitename\n",
    "        TreeFile_SiteName = TreeFile['M/sitename']\n",
    "        \n",
    "        # counting stands\n",
    "        InterCount = 0            \n",
    "        for i in range(len(TreeLat)):\n",
    "            \n",
    "            # Making sure that there is data when and where needed\n",
    "            if (TreeLat[i]>=NN_LatMin-NN_LatDeltaTree and TreeLat[i]<=NN_LatMax+NN_LatDeltaTree and \n",
    "                TreeLon[i]>=NN_LonMin-NN_LonDeltaTree and TreeLon[i]<=NN_LonMax+NN_LonDeltaTree and \n",
    "                # one year lagged chrons are used, the extra year is just for protection, checking for nans here\n",
    "                np.sum(np.isnan(TreeChron[i,NN_TreeYearMin-NN_TreeYearMin:int(NN_TreeYearMax-NN_TreeYearMin+2)]))==0):\n",
    "                \n",
    "                # Need to check if repeated site: \n",
    "                st = TreeFile_SiteName[0][i]\n",
    "                obj = TreeFile[st]\n",
    "                \n",
    "                # mat files are annoying with text :)\n",
    "                str1 = ''.join(chr(i) for i in obj[:])\n",
    "                \n",
    "                # Only use one if duplicate stands are used\n",
    "                if NN_AllowDuplicateStands:\n",
    "                     # storing the TreeSite name\n",
    "                    NN_TreeSite.append(str1)\n",
    "                    \n",
    "                    InterTree = np.array(TreeChron[i,NN_TreeYearMin-NN_TreeYearMin:int(NN_TreeYearMax-NN_TreeYearMin+2)])\n",
    "                    NN_TreeLimData = np.concatenate((NN_TreeLimData,InterTree.reshape(-1,1)),axis=1)\n",
    "                    NN_TreeLimLon = np.concatenate((NN_TreeLimLon,TreeLon[i].reshape(-1,1)),axis=0)\n",
    "                    NN_TreeLimLat = np.concatenate((NN_TreeLimLat,TreeLat[i].reshape(-1,1)),axis=0)\n",
    "                    \n",
    "                    # counting number of stands (print this if you want to know how many have been added)\n",
    "                    InterCount = InterCount+1\n",
    "                    \n",
    "                elif str1 not in NN_TreeSite:\n",
    "                    \n",
    "                    # storing the TreeSite name\n",
    "                    NN_TreeSite.append(str1)\n",
    "                    \n",
    "                    InterTree = np.array(TreeChron[i,NN_TreeYearMin-NN_TreeYearMin:int(NN_TreeYearMax-NN_TreeYearMin+2)])\n",
    "                    NN_TreeLimData = np.concatenate((NN_TreeLimData,InterTree.reshape(-1,1)),axis=1)\n",
    "                    NN_TreeLimLon = np.concatenate((NN_TreeLimLon,TreeLon[i].reshape(-1,1)),axis=0)\n",
    "                    NN_TreeLimLat = np.concatenate((NN_TreeLimLat,TreeLat[i].reshape(-1,1)),axis=0)\n",
    "                    \n",
    "                    # counting number of stands (print this if you want to know how many have been added)\n",
    "                    InterCount = InterCount+1\n",
    "\n",
    "\n",
    "        print(\"%s seconds to run %s\" % (round(time.time() - SecStart,4),SecName))\n",
    "        \n",
    "        # printing the total number of stands\n",
    "        print(str(np.shape(NN_TreeLimData)[1]) + ' trees were found')\n",
    "\n",
    "\n",
    "        SecStart = time.time()\n",
    "        SecName = \"Arrange Data: Trees for each Clim Gridpoint\"\n",
    "\n",
    "        # closest trees\n",
    "        TreeNumClose = np.zeros(len(NN_ClimLimLat))\n",
    "        \n",
    "        # we will store the stands used for each site\n",
    "        NN_ClimTreesVal = []\n",
    "        NN_ClimTreesLat = []\n",
    "        NN_ClimTreesLon = []\n",
    "        NN_ClimTreesInd = []\n",
    "        \n",
    "        # The distance of the farthest stand used for each site\n",
    "        NN_ClimTreesMaxDistUsed = []\n",
    "        for i in range(len(NN_ClimLimLat)):\n",
    "            \n",
    "            # distance of site to the stands\n",
    "            InterDistance = Distance_Earth(NN_ClimLimLat[i]*np.ones(np.shape(NN_TreeLimLat.reshape(-1))),\n",
    "                                                 NN_ClimLimLon[i]*np.ones(np.shape(NN_TreeLimLat.reshape(-1))),\n",
    "                                                 NN_TreeLimLat.reshape(-1), NN_TreeLimLon.reshape(-1))\n",
    "            \n",
    "            # argument of sorted distance\n",
    "            InterDistInd = np.argsort(Distance_Earth(NN_ClimLimLat[i]*np.ones(np.shape(NN_TreeLimLat)),\n",
    "                                                 NN_ClimLimLon[i]*np.ones(np.shape(NN_TreeLimLat)),\n",
    "                                                 NN_TreeLimLat, NN_TreeLimLon).reshape(-1))\n",
    "            # en realidad tengo que checar los árboles que estén cerca de manera en que si nada más se permiten \n",
    "            # 100 árboles pero el 100th árbol está a la misma distancia que el 101, entonces tengo que considerar \n",
    "            # el 101 también, ¿me explico?\n",
    "            \n",
    "            # findinf the distance of the MinTrees stand\n",
    "            InterDistMinTrees = np.sort(Distance_Earth(NN_ClimLimLat[i]*np.ones(np.shape(NN_TreeLimLat)),\n",
    "                                                 NN_ClimLimLon[i]*np.ones(np.shape(NN_TreeLimLat)),\n",
    "                                                 NN_TreeLimLat, NN_TreeLimLon).reshape(-1))[NN_MinTrees-1]\n",
    "            \n",
    "            # checking that the trees are within the absolute maximum distance constraint\n",
    "            if InterDistMinTrees <= NN_MaxDistConstraint:\n",
    "                \n",
    "                # using the maximum number of trees between the ones within the Cook distance used and the MinTrees distance, basically\n",
    "                # if there are two stands with the same distance as the MinTrees stand, then both are used. \n",
    "                InterNumTrees = np.max([np.sum(InterDistance<=NN_MaxDist),np.sum(InterDistance<=InterDistMinTrees)])\n",
    "                \n",
    "                # one year lagged chron stored\n",
    "                InterArrayVal = np.zeros((np.shape(NN_TreeLimData)[0]-1,2*InterNumTrees))\n",
    "                InterArrayLat = np.zeros((2*InterNumTrees))\n",
    "                InterArrayLon = np.zeros((2*InterNumTrees))\n",
    "                InterArrayInd = np.zeros((2*InterNumTrees))\n",
    "                \n",
    "                # storing the data\n",
    "                for j in range(InterNumTrees):\n",
    "                    \n",
    "                    # storing the standardized stand value (assumes that they are the res/std, not the raw stand chronology)\n",
    "                    InterArrayVal[:,j] = ((NN_TreeLimData[0:-1,InterDistInd[j]] - np.mean(NN_TreeLimData[0:-1,InterDistInd[j]]))/\n",
    "                                          np.std(NN_TreeLimData[0:-1,InterDistInd[j]]))\n",
    "                    InterArrayLat[j] = NN_TreeLimLat[InterDistInd[j]]\n",
    "                    InterArrayLon[j] = NN_TreeLimLon[InterDistInd[j]]\n",
    "                    InterArrayInd[j] = InterDistInd[j]\n",
    "                    \n",
    "                    # one year lagged chron added (climate this year can affect the tree next year)\n",
    "                    InterArrayVal[:,InterNumTrees+j] = ((NN_TreeLimData[1:,InterDistInd[j]] - np.mean(NN_TreeLimData[1:,InterDistInd[j]]))/\n",
    "                                                          np.std(NN_TreeLimData[1:,InterDistInd[j]]))\n",
    "                    InterArrayLat[InterNumTrees+j] = NN_TreeLimLat[InterDistInd[j]]\n",
    "                    InterArrayLon[InterNumTrees+j] = NN_TreeLimLon[InterDistInd[j]]\n",
    "                    InterArrayInd[InterNumTrees+j] = InterDistInd[j]\n",
    "                \n",
    "                # appending these to the full record. \n",
    "                NN_ClimTreesVal.append(InterArrayVal)\n",
    "                NN_ClimTreesLat.append(InterArrayLat)\n",
    "                NN_ClimTreesLon.append(InterArrayLon)\n",
    "                NN_ClimTreesInd.append(InterArrayInd)\n",
    "                NN_ClimTreesMaxDistUsed.append(InterDistMinTrees)\n",
    "                TreeNumClose[i] = np.shape(InterArrayVal)[1]\n",
    "            \n",
    "            # this site is basically not used, as the distance of the MinTree's stand is farther than absolute max distance\n",
    "            else:\n",
    "\n",
    "                NN_ClimTreesVal.append(np.zeros((np.shape(NN_TreeLimData)[0]-1,2*NN_MinTrees)))\n",
    "                NN_ClimTreesLat.append(np.zeros((2*NN_MinTrees))*np.nan)\n",
    "                NN_ClimTreesLon.append(np.zeros((2*NN_MinTrees))*np.nan)\n",
    "                NN_ClimTreesInd.append(np.zeros((2*NN_MinTrees))*np.nan)\n",
    "                TreeNumClose[i] = 2*NN_MinTrees\n",
    "\n",
    "        \n",
    "\n",
    "        print(\"%s seconds to run %s\" % (round(time.time() - SecStart,4),SecName))\n",
    "        text = (\"%s minutes to run %s\" % (round((time.time() - SecStart)/60,4),SecName))\n",
    "        csv_status_file.append(text)\n",
    "        with open(txt_file_name, 'w') as f:\n",
    "            for item in csv_status_file:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "        # prewhitening and standardizing climate. \n",
    "        SecStart = time.time()\n",
    "        SecName = \"Data: Prewhiten and standardize climate\"\n",
    "        \n",
    "        # AR refers to the prewhitening\n",
    "        NN_ClimARData = np.zeros((np.shape(NN_ClimLimData)[0]-NN_ARTimeDelta,np.shape(NN_ClimLimData)[1]))\n",
    "        NN_ClimARData.fill(np.nan)\n",
    "        \n",
    "        # the actual coefficients that are found\n",
    "        NN_ClimARCoef = np.zeros((NN_ARPMax,np.shape(NN_ClimLimData)[1]))\n",
    "        NN_ClimARCoef.fill(np.nan)\n",
    "        \n",
    "        # storing the mean and std of the climate over the training period, added back at the end\n",
    "        NN_ClimARCaliMean = np.zeros(np.shape(NN_ClimLimLat))\n",
    "        NN_ClimARCaliMean.fill(np.nan)\n",
    "        NN_ClimARCaliStdev = np.zeros(np.shape(NN_ClimLimLat))\n",
    "        NN_ClimARCaliStdev.fill(np.nan)\n",
    "\n",
    "        # I love counting things, as has become obvious. \n",
    "        InterCount = 0\n",
    "        for i in range(len(NN_ClimLimLat)):\n",
    "            \n",
    "            # AR of climate\n",
    "            InterClimAR = AutoRegression(NN_ClimLimData[:,i], NN_ARPMin, NN_ARPMax, AICcOrBIC='AIC')\n",
    "            \n",
    "            # storing the prewhitened data (but avoiding first years)\n",
    "            NN_ClimARData[:,i] = InterClimAR.ARTimeSeries[NN_ARTimeDelta:].reshape(-1)\n",
    "            \n",
    "            # storing the coefficients found\n",
    "            NN_ClimARCoef[0:InterClimAR.ARP,i] = InterClimAR.ARCoefs.reshape(-1)\n",
    "            \n",
    "            # if randomized training and testing\n",
    "            if NN_UseRandTrainTest != 0:\n",
    "                clim_tr, clim_va, clim_tr, clim_va = train_test_split(NN_ClimARData[:,i],\n",
    "                                                          NN_ClimARData[:,i], \n",
    "                                                          test_size=1-NN_UseRandTrainTest, random_state=NN_SeedNo)\n",
    "                NN_ClimARCaliMean[i] = np.mean(clim_tr.reshape(-1))\n",
    "                NN_ClimARCaliStdev[i] = np.std(clim_tr.reshape(-1))\n",
    "            # if not using randomized training and testing, I personally do not like this, results can be contingent on combo of years used. \n",
    "            else:\n",
    "                NN_ClimARCaliMean[i] = np.mean(NN_ClimARData[NN_CaliYearMin-NN_YearMin:NN_CaliYearMax-NN_YearMin+1,i].reshape(-1))\n",
    "                NN_ClimARCaliStdev[i] = np.std(NN_ClimARData[NN_CaliYearMin-NN_YearMin:NN_CaliYearMax-NN_YearMin+1,i].reshape(-1))\n",
    "            \n",
    "            # count count count\n",
    "            InterCount = InterCount+1    \n",
    "\n",
    "        # standardizing the result. \n",
    "        if NN_StandardizeBeforeMethod:\n",
    "            NN_ClimARStd = np.multiply(NN_ClimARData-np.tile(NN_ClimARCaliMean.reshape(1,-1),(NN_YearMax-NN_YearMin+1,1)),\n",
    "                                        1./np.tile(NN_ClimARCaliStdev.reshape(1,-1),(NN_YearMax-NN_YearMin+1,1)))\n",
    "        else:\n",
    "            NN_ClimARStd = NN_ClimARData\n",
    "            NN_ClimARCaliMean = np.zeros(np.shape(NN_ClimARCaliMean))\n",
    "            NN_ClimARCaliStdev = np.ones(np.shape(NN_ClimARCaliStdev))\n",
    "            \n",
    "\n",
    "        print(\"%s seconds to run %s\" % (round(time.time() - SecStart,4),SecName)) \n",
    "        text = (\"%s minutes to run %s\" % (round((time.time() - SecStart)/60,4),SecName))\n",
    "        csv_status_file.append(text)\n",
    "        with open(txt_file_name, 'w') as f:\n",
    "            for item in csv_status_file:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "        # # Neural Network Start\n",
    "\n",
    "\n",
    "        SecStart = time.time() # 10,000 scs por lugar. o 4,800 checar por qué la diferencia\n",
    "\n",
    "        SecName = \"Neural Nets Start\"\n",
    "        \n",
    "\n",
    "        # WOOOOOHOOO STARTING NN and PCA\n",
    "        NN_NNModels = []\n",
    "        if NN_UseRandTrainTest ==0:\n",
    "            \n",
    "            # training record (calibration)\n",
    "            NN_ReconClimARStdCali = np.zeros((NN_CaliYearMax - NN_CaliYearMin + 1, len(NN_ClimTreesVal)))\n",
    "            NN_ReconClimARStdCali.fill(np.nan)\n",
    "            \n",
    "            # testing record (validation)\n",
    "            NN_ReconClimARStdVali = np.zeros((NN_ValiYearMax - NN_ValiYearMin + 1, len(NN_ClimTreesVal)))\n",
    "            NN_ReconClimARStdVali.fill(np.nan)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # size of training and testing being defined. (CHECK THIS IN CASE THERE WAS A ROUNDING ERROR)\n",
    "            y = NN_ClimARStd[:,0].reshape(-1,1)\n",
    "            x = NN_ClimTreesVal[0][NN_YearMin-NN_TreeYearMin:NN_YearMax-NN_TreeYearMin+1,:]\n",
    "            x_tr, x_va, y_tr, y_va = train_test_split(x, y, test_size=1-NN_UseRandTrainTest, random_state=NN_SeedNo) #42\n",
    "\n",
    "            # same as before\n",
    "            NN_ReconClimARStdCali = np.zeros((len(y_tr), len(NN_ClimTreesVal)))\n",
    "            NN_ReconClimARStdCali.fill(np.nan)\n",
    "            NN_ReconClimARStdVali = np.zeros((len(y_va), len(NN_ClimTreesVal)))\n",
    "            NN_ReconClimARStdVali.fill(np.nan)\n",
    "\n",
    "        # full record\n",
    "        NN_ReconClimARStdTotal = np.zeros((np.shape(NN_ClimTreesVal[1])[0],len(NN_ClimTreesVal)))\n",
    "        NN_ReconClimARStdTotal.fill(np.nan)\n",
    "\n",
    "        # climate range reconstruction\n",
    "        NN_ReconClimARStdClimRange = np.zeros(np.shape(NN_ClimARStd))\n",
    "        NN_ReconClimARStdClimRange.fill(np.nan)\n",
    "\n",
    "\n",
    "        \n",
    "        # FOR PCA (MLR here)\n",
    "        MLR_ReconTreesUsed = np.zeros(len(NN_ClimLimLat))\n",
    "        MLR_ReconTreesUsed.fill(np.nan)\n",
    "        MLR_ReconCoefs = []\n",
    "        \n",
    "        # Same as NN but for MLR/PCA\n",
    "        MLR_ReconClimARCaliStd = np.zeros(np.shape(NN_ReconClimARStdCali))\n",
    "        MLR_ReconClimARCaliStd.fill(np.nan)\n",
    "        MLR_ReconClimARValiStd = np.zeros(np.shape(NN_ReconClimARStdVali))\n",
    "        MLR_ReconClimARValiStd.fill(np.nan)\n",
    "        MLR_ReconClimARTotalStd = np.zeros(np.shape(NN_ReconClimARStdTotal))\n",
    "        MLR_ReconClimARTotalStd.fill(np.nan)\n",
    "        MLR_ReconClimARClimRangeStd = np.zeros(np.shape(NN_ClimARStd))\n",
    "        MLR_ReconClimARClimRangeStd.fill(np.nan)\n",
    "        \n",
    "        # Eigenvectors used (from Cook et al (1999))\n",
    "        MLR_ReconTreeCorrEigVecUsed = np.zeros((len(NN_ClimLimLat),int(np.max(TreeNumClose)),np.shape(NN_ReconClimARStdCali)[0]))\n",
    "        MLR_ReconTreeCorrEigVecUsed.fill(np.nan)\n",
    "        \n",
    "        # significantly correlated number of stands used for MLR \n",
    "        TreeGridCorrClose = np.zeros(len(NN_ClimLimLat))\n",
    "\n",
    "        # storing results (at times, I love redundancy)\n",
    "        NN_ClimMinTrees = []\n",
    "        NN_ReconClimMinTrees = []\n",
    "        NN_ClimMaxTrees = []\n",
    "        NN_ReconClimMaxTrees = []\n",
    "        \n",
    "        # for sensitivity, nor used much Sprt = separate. , These are not saved if SaveBasicResults is 1\n",
    "        NN_SensReconClimAllAdd_Sprt_Sens = []\n",
    "        NN_SensReconClimAllAdd_Sprt_x = []\n",
    "        NN_SensReconClimAllAdd_Sprt_y = []\n",
    "        NN_SensReconClimAllAdd_Mean_Sens = []\n",
    "        NN_SensReconClimAllAdd_Mean_x = []\n",
    "        NN_SensReconClimAllAdd_Group_x = []\n",
    "        NN_SensReconClimAllAdd_Group_y = []\n",
    "        NN_SensReconClimAllAdd_Group_Same_x = []\n",
    "        NN_SensReconClimAllAdd_Group_Same_y = []\n",
    "        \n",
    "        # used lat and lon for reconstruction\n",
    "        NN_ReconClimTreesLatUsed = []\n",
    "        NN_ReconClimTreesLonUsed = []\n",
    "\n",
    "        # storing this just in case. \n",
    "        NN_HiddenNodesUsed = []\n",
    "\n",
    "        # storing these just in case\n",
    "        NN_DropoutUsed = []\n",
    "        NN_RegWeightUsed = []    \n",
    "\n",
    "        # Sort of like Count von Count (Sesame St)\n",
    "        InterCount = 0\n",
    "        \n",
    "        # Correlation which is significant considering the years used for training\n",
    "        MLR_SigCorrel = SigCorrel(np.shape(NN_ReconClimARStdCali)[0]-2, MLR_AlphaVal)\n",
    "        for i in range(len(NN_ClimLimLat)):\n",
    "            if NN_UseRandTrainTest == 0:\n",
    "                # use defined intervals for train and test\n",
    "                x_tr = NN_ClimTreesVal[i][NN_CaliYearMin-NN_TreeYearMin:NN_CaliYearMax-NN_TreeYearMin+1,:]\n",
    "                y_tr = NN_ClimARStd[NN_CaliYearMin-NN_YearMin:NN_CaliYearMax-NN_YearMin+1,i].reshape(-1,1)\n",
    "                x_va = NN_ClimTreesVal[i][NN_ValiYearMin-NN_TreeYearMin:NN_ValiYearMax-NN_TreeYearMin+1,:]\n",
    "                y_va = NN_ClimARStd[NN_ValiYearMin-NN_ClimYearMin:NN_ValiYearMax-NN_ClimYearMin+1,i].reshape(-1,1)\n",
    "                \n",
    "            else:\n",
    "                # random train test intervals\n",
    "                y = NN_ClimARStd[:,i].reshape(-1,1)\n",
    "                x = NN_ClimTreesVal[i][NN_YearMin-NN_TreeYearMin:NN_YearMax-NN_TreeYearMin+1,:]\n",
    "                x_tr, x_va, y_tr, y_va = train_test_split(x, y, test_size=1-NN_UseRandTrainTest, random_state=NN_SeedNo) #Random before 42\n",
    "\n",
    "            # lat and lon for the trees\n",
    "            x_lat = NN_ClimTreesLat[i]\n",
    "            x_lon = NN_ClimTreesLon[i]\n",
    "            x_to = NN_ClimTreesVal[i]\n",
    "            \n",
    "            # only the range that you are comparing to the climate variable\n",
    "            x_ClimRange = NN_ClimTreesVal[i][NN_YearMin-NN_TreeYearMin:NN_YearMax-NN_TreeYearMin+1,:]\n",
    "            \n",
    "            # making the arrays that will be filled\n",
    "            MLR_x_tr = np.zeros(np.shape(x_tr))\n",
    "            MLR_x_va = np.zeros(np.shape(x_va))\n",
    "            MLR_x_to = np.zeros(np.shape(x_to))\n",
    "            MLR_x_ClimRange = np.zeros(np.shape(x_ClimRange))\n",
    "            MLR_x_lat = np.zeros(np.shape(x_lat))\n",
    "            MLR_x_lon = np.zeros(np.shape(x_lon))\n",
    "            MLR_x_tr.fill(np.nan)\n",
    "            MLR_x_va.fill(np.nan)\n",
    "            MLR_x_to.fill(np.nan)\n",
    "            MLR_x_ClimRange.fill(np.nan)\n",
    "            MLR_x_lat.fill(np.nan)\n",
    "            MLR_x_lon.fill(np.nan)\n",
    "            \n",
    "            #Level 2 of Cook et al: only those that are significantly correlated to scPDSI at the gridpoint will be kept\n",
    "            InterCountCorr=0\n",
    "            for jCorr in range(int(TreeNumClose[i])):\n",
    "                if np.abs(np.corrcoef(x_tr[:,jCorr],y_tr.reshape(-1))[0,1]) >= MLR_SigCorrel:\n",
    "                    MLR_x_tr[:,InterCountCorr] = x_tr[:,jCorr]\n",
    "                    MLR_x_va[:,InterCountCorr] = x_va[:,jCorr]\n",
    "                    MLR_x_to[:,InterCountCorr] = x_to[:,jCorr]\n",
    "                    MLR_x_ClimRange[:,InterCountCorr] = x_ClimRange[:,jCorr]\n",
    "                    MLR_x_lat[InterCountCorr] = NN_ClimTreesLat[i][jCorr]\n",
    "                    MLR_x_lon[InterCountCorr] = NN_ClimTreesLon[i][jCorr]\n",
    "                    InterCountCorr = InterCountCorr + 1      \n",
    "            \n",
    "            TreeGridCorrClose[i] = InterCountCorr\n",
    "            MLR_x_tr = MLR_x_tr[:,0:InterCountCorr]\n",
    "            MLR_x_va = MLR_x_va[:,0:InterCountCorr]\n",
    "            MLR_x_to = MLR_x_to[:,0:InterCountCorr]\n",
    "            MLR_x_ClimRange = MLR_x_ClimRange[:,0:InterCountCorr]\n",
    "            MLR_x_lat = MLR_x_lat[0:InterCountCorr]\n",
    "            MLR_x_lon = MLR_x_lon[0:InterCountCorr]\n",
    "            \n",
    "            # In order to use the same inputs in the NN as with the MLR \n",
    "            # (only the significantly correlated stands to target during train period. )\n",
    "            if NN_UseSameInputMLR == 1:\n",
    "                x_tr = copy.deepcopy(MLR_x_tr)\n",
    "                x_va = copy.deepcopy(MLR_x_va)\n",
    "                x_to = copy.deepcopy(MLR_x_to)\n",
    "                x_ClimRange = copy.deepcopy(MLR_x_ClimRange)\n",
    "                x = copy.deepcopy(MLR_x_ClimRange)\n",
    "                x_lat = copy.deepcopy(MLR_x_lat)\n",
    "                x_lon = copy.deepcopy(MLR_x_lon)\n",
    "                \n",
    "            \n",
    "            # making sure that at least one stand chronology is correlated to the scPDSI\n",
    "            if InterCountCorr > 0:\n",
    "            \n",
    "                # All of theis codfe is in the original MLR_NN code\n",
    "                \n",
    "                # Running the PCA (described in Cook et al (1999) if a reference is needed)\n",
    "                MLR_TreeCaliForPCA = MLR_x_tr.T\n",
    "                MLR_TreeCorrMatrix = np.corrcoef(MLR_TreeCaliForPCA,rowvar=True)\n",
    "                if np.shape(MLR_TreeCorrMatrix) == ():\n",
    "                    MLR_TreeCorrEigValMat, MLR_TreeCorrEigVecMat = np.linalg.eig(MLR_TreeCorrMatrix.reshape(-1,1))\n",
    "                else:\n",
    "                    MLR_TreeCorrEigValMat, MLR_TreeCorrEigVecMat = np.linalg.eig(MLR_TreeCorrMatrix)\n",
    "               \n",
    "                # Checking how many factors satisfy the Kaiser Guttman criterion (eigenvalue>=1)\n",
    "                MLR_TreeKaisGutt = np.sum(MLR_TreeCorrEigValMat>=1)\n",
    "                \n",
    "                # sort in decreasing eigenvalue \n",
    "                MLR_TreeCorrEigVecSort = MLR_TreeCorrEigVecMat[:,np.argsort(-MLR_TreeCorrEigValMat)]\n",
    "                \n",
    "                # keeping only those that satisfy the criterion\n",
    "                MLR_TreeCorrEigVecKG = MLR_TreeCorrEigVecSort[:,0:MLR_TreeKaisGutt]\n",
    "                \n",
    "                # calculating the scores\n",
    "                MLR_TreeScores = np.dot(MLR_TreeCaliForPCA.T,MLR_TreeCorrEigVecKG)\n",
    "                \n",
    "                # making the array\n",
    "                MLR_ClimCorrTreeScore = np.zeros(np.shape(MLR_TreeScores[1]))\n",
    "                \n",
    "                # need to find which scores are best correlated (in absolute value) to the target scPDSI\n",
    "                for j in range(len(MLR_ClimCorrTreeScore)):\n",
    "                    MLR_ClimCorrTreeScore[j] = np.abs(np.corrcoef(y_tr.reshape(-1),MLR_TreeScores[:,j])[0,1])\n",
    "                \n",
    "                # sorted by absolute correlation\n",
    "                MLR_TreeScoresSort = MLR_TreeScores[:,np.argsort(-np.abs(MLR_ClimCorrTreeScore))]\n",
    "                \n",
    "                # storing the eigenvectors\n",
    "                MLR_TreeCorrEigVecKGSort = MLR_TreeCorrEigVecKG[:,np.argsort(-np.abs(MLR_ClimCorrTreeScore))]\n",
    "                \n",
    "                # regression using the AICc criterion\n",
    "                MLR_ReconClimCali, MLR_ReconClimBetaCoef, MLR_ReconCaliAICcMin = RegressAICc(np.real(MLR_TreeScoresSort),\n",
    "                                                                                                     np.real(y_tr).reshape(-1))\n",
    "                \n",
    "                # storing how many are used\n",
    "                MLR_ReconTreesUsed[i] = MLR_ReconCaliAICcMin\n",
    "                \n",
    "                # storing the coefficients\n",
    "                MLR_ReconCoefs.append(MLR_ReconClimBetaCoef.reshape(-1))\n",
    "                \n",
    "                # Storing the AR Std calibration reconstruction\n",
    "                MLR_ReconClimARCaliStd[:,i] = MLR_ReconClimCali.reshape(-1)\n",
    "                \n",
    "                # storing the eigvecs used\n",
    "                MLR_ReconTreeCorrEigVecUsed[i,0:int(TreeGridCorrClose[i]),0:int(MLR_ReconCaliAICcMin)] = MLR_TreeCorrEigVecKGSort[:,0:MLR_ReconCaliAICcMin]\n",
    "                \n",
    "                # regression for testing\n",
    "                MLR_ReconClimARValiStd[:,i] = np.dot(np.dot(MLR_x_va,\n",
    "                                                        MLR_TreeCorrEigVecKGSort[:,0:MLR_ReconCaliAICcMin]),\n",
    "                                                 MLR_ReconClimBetaCoef.reshape(-1,1)).reshape(-1)\n",
    "                \n",
    "                # full regression (full in terms of stands)\n",
    "                MLR_ReconClimARTotalStd[:,i] = np.dot(np.dot(MLR_x_to,\n",
    "                                                         MLR_TreeCorrEigVecKGSort[:,0:MLR_ReconCaliAICcMin]),\n",
    "                                                  MLR_ReconClimBetaCoef.reshape(-1,1)).reshape(-1)\n",
    "                \n",
    "                # climate range regression\n",
    "                MLR_ReconClimARClimRangeStd[:,i] = np.dot(np.dot(MLR_x_ClimRange,\n",
    "                                                                 MLR_TreeCorrEigVecKGSort[:,0:MLR_ReconCaliAICcMin]),\n",
    "                                                          MLR_ReconClimBetaCoef.reshape(-1,1)).reshape(-1)\n",
    "                \n",
    "                # Storing the relevant scores\n",
    "                MLR_TreeScoresCali = np.dot(MLR_x_tr,MLR_TreeCorrEigVecKGSort[:,0:MLR_ReconCaliAICcMin])\n",
    "                MLR_TreeScoresVali = np.dot(MLR_x_va,MLR_TreeCorrEigVecKGSort[:,0:MLR_ReconCaliAICcMin])\n",
    "                MLR_TreeScoresTotal = np.dot(MLR_x_to,MLR_TreeCorrEigVecKGSort[:,0:MLR_ReconCaliAICcMin])\n",
    "                MLR_TreeScoresClimRange = np.dot(MLR_x_ClimRange,MLR_TreeCorrEigVecKGSort[:,0:MLR_ReconCaliAICcMin])\n",
    "\n",
    "            \n",
    "            # To make sure that it is valid to perform the NN -- cosidering the NN_UseSameInputMLR\n",
    "            # basically to make sure that there are enough stands to perform NN\n",
    "            if NN_UseSameInputMLR == 1:\n",
    "                \n",
    "                if InterCountCorr > 0:\n",
    "                    IndexCondition = 1\n",
    "                else:\n",
    "                    IndexCondition = 0\n",
    "            else: \n",
    "                IndexCondition = 1\n",
    "            if IndexCondition == 1:\n",
    "                \n",
    "                # making the architecture of the NN be made depending on the data available \n",
    "                # basically the number of fitted parameters is a function of the stand-year data amount \n",
    "                # so as to not have more coefficients fit than data available, \n",
    "                # depends on NN_UseAdaptiveNumNodes (proportion of data number used)\n",
    "                if NN_UseAdaptiveNumNodes != 0:\n",
    "                    \n",
    "                    NN_HiddenNodes = int(np.floor(np.max(np.roots([NN_HiddenLayers-1,\n",
    "                                                                   np.shape(x_tr)[1]+1+NN_HiddenLayers,\n",
    "                                                                   1-(np.shape(x_tr)[1])*(np.shape(x_tr)[0])*NN_UseAdaptiveNumNodes]))))\n",
    "                # Storing the number of nodes\n",
    "                NN_HiddenNodesUsed.append(NN_HiddenNodes)\n",
    "                \n",
    "                # Storing the regularization\n",
    "                NN_DropoutUsed.append(NN_Dropout)\n",
    "                NN_RegWeightUsed.append(NN_RegWeight)\n",
    "                \n",
    "                # parameters used for the NN\n",
    "                NN_Params = [NN_HiddenLayers,NN_HiddenNodes,NN_Activation,NN_RegWeight,[NN_DropoutInput,NN_Dropout,NN_DropoutMod],NN_TrainEpochs]\n",
    "                \n",
    "                # WOOOOOHOOOO NN USED. \n",
    "                Model_Results = NN_Calc(x_tr,y_tr,x_va,x_to,x_ClimRange,NN_Params)\n",
    "    \n",
    "     \n",
    "            # this is the output\n",
    "            #     NN_Calc returns: [Model, #0\n",
    "            #             y_ReconTr, #1\n",
    "            #             y_ReconVa, #2 \n",
    "            #             y_ReconTo, #3\n",
    "            #             y_ReconOther, #4\n",
    "            #             NN_Sens_Group, #5\n",
    "            #             NN_Sens_Indiv_Mean, #6\n",
    "            #             NN_Sens_Indiv_Sprt, #7\n",
    "            #.            NN_Sens_Group_Same] #8\n",
    "                \n",
    "            # Save the model? Uses some storage. \n",
    "            if NN_SaveModels == 1: \n",
    "                Model = Model_Results[0]\n",
    "                InterTime = copy.deepcopy(dt.datetime.now().strftime(\"%Y%m%d-%H\")[2:]) #this in order to avoid changes in hous between the saves. \n",
    "                Model.save('./Results/Python/NNTrees/Models/'+\n",
    "                           InterTime+'_NNModel_GridPoint-'+str(int(i))+'_Rnd-'+str(iTestRnd)+'_Reg-'+str(iTestReg)+'.h5')\n",
    "                NN_NNModels.append('./Results/Python/NNTrees/Models/'+\n",
    "                           InterTime+'_NNModel_GridPoint-'+str(int(i))+'_Rnd-'+str(iTestRnd)+'_Reg-'+str(iTestReg)+'.h5')\n",
    "                Model.save('./Results/Python/NNTrees/Models/'+\n",
    "                           'NNModel_GridPoint-'+str(int(i))+'_Rnd-'+str(iTestRnd)+'_Reg-'+str(iTestReg)+'.h5') \n",
    "\n",
    "                Model_yaml = Model.to_yaml()\n",
    "                with open('./Results/Python/NNTrees/Models/'+\n",
    "                           InterTime+'_YAML-NNModel_GridPoint-'+str(int(i))+'_Rnd-'+str(iTestRnd)+'_Reg-'+str(iTestReg)+'.yaml', \"w\") as yaml_file:\n",
    "                    yaml_file.write(Model_yaml)    \n",
    "            \n",
    "            # the predictions for the total, the train, the test, and theother range. \n",
    "            NN_ReconClimARStdTotal[:,i] = Model_Results[3].reshape(-1)#Y_ReconTot.reshape(-1)\n",
    "            NN_ReconClimARStdCali[:,i] = Model_Results[1].reshape(-1)#Y_ReconCali.reshape(-1)\n",
    "            NN_ReconClimARStdVali[:,i] = Model_Results[2].reshape(-1)#Y_ReconVali.reshape(-1)\n",
    "            NN_ReconClimARStdClimRange[:,i] = Model_Results[4].reshape(-1)#Y_ReconTot[NN_YearMin - NN_TreeYearMin: NN_YearMax - NN_TreeYearMin + 1,0].reshape(-1)\n",
    "            \n",
    "            # Each tree is perturbes separately and the Delta target is recorded, ad well as the background target \n",
    "            NN_SensReconClimAllAdd_Sprt_Sens.append(Model_Results[7][1])\n",
    "            NN_SensReconClimAllAdd_Sprt_x.append(Model_Results[7][0])\n",
    "            NN_SensReconClimAllAdd_Sprt_y.append(Model_Results[7][2])\n",
    "            \n",
    "            # Each tree goes through its range of growth andthe rest of the stands are kept at their mean.\n",
    "            NN_SensReconClimAllAdd_Mean_Sens.append(Model_Results[6][1])\n",
    "            NN_SensReconClimAllAdd_Mean_x.append(Model_Results[6][0])\n",
    "            \n",
    "            # All trees are input sorted by their ARSTAN growth \n",
    "            NN_SensReconClimAllAdd_Group_x.append(Model_Results[5][0])\n",
    "            NN_SensReconClimAllAdd_Group_y.append(Model_Results[5][1])\n",
    "            \n",
    "            NN_SensReconClimAllAdd_Group_Same_x.append(Model_Results[8][0])\n",
    "            NN_SensReconClimAllAdd_Group_Same_y.append(Model_Results[8][1])\n",
    "            \n",
    "            # lat on of. thestands used or the sensitivity analysis\n",
    "            NN_ReconClimTreesLatUsed.append(x_lat)\n",
    "            NN_ReconClimTreesLonUsed.append(x_lon)\n",
    "\n",
    "            InterCount = InterCount+1    \n",
    "            if InterCount%5 == 0:\n",
    "                print(\"%s seconds to run %s of %s places\" % (round(time.time() - SecStart,4),str(InterCount),str(len(NN_ClimLimLat))))\n",
    "                text = (\"%s minutes to run %s of %s places\" % (round((time.time() - SecStart)/60,4),str(InterCount),str(len(NN_ClimLimLat))))\n",
    "                csv_status_file.append(text)\n",
    "                with open(txt_file_name, 'w') as f:\n",
    "                    for item in csv_status_file:\n",
    "                        f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "\n",
    "        if NN_StandardizeBeforeMethod:\n",
    "            # Storing all of the results (Train, test, total, climate range) adding back the calibration stdev and mean\n",
    "            #  This assumes that the output has zero mean, for example, which might not be true. \n",
    "            NN_ReconClimARCali = copy.deepcopy((np.multiply(NN_ReconClimARStdCali,np.tile(NN_ClimARCaliStdev.reshape(1,-1),(len(x_tr),1)))+\n",
    "                               np.tile(NN_ClimARCaliMean.reshape(1,-1),(len(x_tr),1))))\n",
    "            NN_ReconClimARVali = copy.deepcopy((np.multiply(NN_ReconClimARStdVali,np.tile(NN_ClimARCaliStdev.reshape(1,-1),(len(x_va),1)))+\n",
    "                               np.tile(NN_ClimARCaliMean.reshape(1,-1),(len(x_va),1))))\n",
    "            NN_ReconClimARTotal = copy.deepcopy((np.multiply(NN_ReconClimARStdTotal,np.tile(NN_ClimARCaliStdev.reshape(1,-1),(np.shape(NN_ClimTreesVal[1])[0],1)))+\n",
    "                               np.tile(NN_ClimARCaliMean.reshape(1,-1),(np.shape(NN_ClimTreesVal[1])[0],1))))\n",
    "            NN_ReconClimARClimRange = copy.deepcopy((np.multiply(NN_ReconClimARStdClimRange,np.tile(NN_ClimARCaliStdev.reshape(1,-1),(np.shape(NN_ReconClimARStdClimRange)[0],1)))+\n",
    "                               np.tile(NN_ClimARCaliMean.reshape(1,-1),(np.shape(NN_ReconClimARStdClimRange)[0],1))))\n",
    "\n",
    "            # Storing the MLR equivalents.\n",
    "            MLR_ReconClimARCali = copy.deepcopy((np.multiply(MLR_ReconClimARCaliStd,np.tile(NN_ClimARCaliStdev.reshape(1,-1),(len(x_tr),1)))+\n",
    "                               np.tile(NN_ClimARCaliMean.reshape(1,-1),(len(x_tr),1))))\n",
    "            MLR_ReconClimARVali = copy.deepcopy((np.multiply(MLR_ReconClimARValiStd,np.tile(NN_ClimARCaliStdev.reshape(1,-1),(len(x_va),1)))+\n",
    "                               np.tile(NN_ClimARCaliMean.reshape(1,-1),(len(x_va),1))))\n",
    "            MLR_ReconClimARTotal = copy.deepcopy((np.multiply(MLR_ReconClimARTotalStd,np.tile(NN_ClimARCaliStdev.reshape(1,-1),(np.shape(NN_ClimTreesVal[1])[0],1)))+\n",
    "                               np.tile(NN_ClimARCaliMean.reshape(1,-1),(np.shape(NN_ClimTreesVal[1])[0],1))))\n",
    "            MLR_ReconClimARClimRange = copy.deepcopy((np.multiply(MLR_ReconClimARClimRangeStd,np.tile(NN_ClimARCaliStdev.reshape(1,-1),(np.shape(NN_ReconClimARStdClimRange)[0],1)))+\n",
    "                               np.tile(NN_ClimARCaliMean.reshape(1,-1),(np.shape(NN_ReconClimARStdClimRange)[0],1))))\n",
    "        else:\n",
    "            \n",
    "            NN_ReconClimARCali = copy.deepcopy(NN_ReconClimARStdCali)\n",
    "            NN_ReconClimARVali = copy.deepcopy(NN_ReconClimARStdVali)\n",
    "            NN_ReconClimARTotal = copy.deepcopy(NN_ReconClimARStdTotal)\n",
    "            NN_ReconClimARClimRange = copy.deepcopy(NN_ReconClimARStdClimRange)\n",
    "\n",
    "            # Storing the MLR equivalents.\n",
    "            MLR_ReconClimARCali = copy.deepcopy(MLR_ReconClimARCaliStd)\n",
    "            MLR_ReconClimARVali = copy.deepcopy(MLR_ReconClimARValiStd)\n",
    "            MLR_ReconClimARTotal = copy.deepcopy(MLR_ReconClimARTotalStd)\n",
    "            MLR_ReconClimARClimRange = copy.deepcopy(MLR_ReconClimARClimRangeStd)\n",
    "\n",
    "        NN_ReconClimCali = np.zeros(np.shape(NN_ReconClimARCali))\n",
    "        NN_ReconClimVali = np.zeros(np.shape(NN_ReconClimARVali))\n",
    "        NN_ClimDataCali = np.zeros(np.shape(NN_ReconClimARCali))\n",
    "        NN_ClimDataVali = np.zeros(np.shape(NN_ReconClimARVali))\n",
    "        NN_ReconClimCali.fill(np.nan)\n",
    "        NN_ReconClimVali.fill(np.nan)\n",
    "        NN_ClimDataCali.fill(np.nan)\n",
    "        NN_ClimDataVali.fill(np.nan)\n",
    "\n",
    "\n",
    "        NN_ReconClimTotal = np.zeros(np.shape(NN_ReconClimARTotal))\n",
    "        NN_ReconClimTotal.fill(np.nan)\n",
    "        \n",
    "        MLR_ReconClimCali = np.zeros(np.shape(NN_ReconClimARCali))\n",
    "        MLR_ReconClimVali = np.zeros(np.shape(NN_ReconClimARVali))\n",
    "        MLR_ReconClimCali.fill(np.nan)\n",
    "        MLR_ReconClimVali.fill(np.nan)\n",
    "        MLR_ReconClimTotal = np.zeros(np.shape(NN_ReconClimARTotal))\n",
    "        MLR_ReconClimTotal.fill(np.nan)\n",
    "\n",
    "        # Adding back the redness that was removed from climate\n",
    "        for i in range(len(NN_ClimLimLat)):\n",
    "            NN_InterTimeSeries = copy.deepcopy(NN_ReconClimARTotal[:,i].reshape(-1,1))\n",
    "            MLR_InterTimeSeries = copy.deepcopy(MLR_ReconClimARTotal[:,i].reshape(-1,1))\n",
    "            NN_ReconClimTotal[:,i] = InvAR(NN_InterTimeSeries,NN_ClimARCoef[0:np.sum(~np.isnan(NN_ClimARCoef[:,i])),i]).reshape(-1)\n",
    "            MLR_ReconClimTotal[:,i] = InvAR(MLR_InterTimeSeries,NN_ClimARCoef[0:np.sum(~np.isnan(NN_ClimARCoef[:,i])),i]).reshape(-1)\n",
    "            \n",
    "            # getting the training and testing considering the redness\n",
    "            if NN_UseRandTrainTest != 0:\n",
    "                NN_climrec_tr, NN_climrec_va, clim_tr, clim_va = train_test_split(NN_ReconClimTotal[NN_YearMin - NN_TreeYearMin:NN_YearMax - NN_TreeYearMin+1,i],\n",
    "                                                       NN_ClimDataToStat[:,i], \n",
    "                                                       test_size=1-NN_UseRandTrainTest, random_state=NN_SeedNo)\n",
    "                MLR_climrec_tr, MLR_climrec_va, clim_tr, clim_va = train_test_split(MLR_ReconClimTotal[NN_YearMin - NN_TreeYearMin:NN_YearMax - NN_TreeYearMin+1,i],\n",
    "                                                       NN_ClimDataToStat[:,i], \n",
    "                                                       test_size=1-NN_UseRandTrainTest, random_state=NN_SeedNo)\n",
    "                NN_ReconClimCali[:,i] = NN_climrec_tr\n",
    "                NN_ReconClimVali[:,i] = NN_climrec_va\n",
    "                MLR_ReconClimCali[:,i] = MLR_climrec_tr\n",
    "                MLR_ReconClimVali[:,i] = MLR_climrec_va\n",
    "                NN_ClimDataCali[:,i] = clim_tr\n",
    "                NN_ClimDataVali[:,i] = clim_va\n",
    "\n",
    "\n",
    "        if NN_UseRandTrainTest == 0:\n",
    "            NN_ReconClimCali = NN_ReconClimTotal[NN_CaliYearMin-NN_TreeYearMin:NN_CaliYearMax-NN_TreeYearMin+1,:]\n",
    "            NN_ReconClimVali = NN_ReconClimTotal[NN_ValiYearMin-NN_TreeYearMin:NN_ValiYearMax-NN_TreeYearMin+1,:]\n",
    "            MLR_ReconClimCali = MLR_ReconClimTotal[NN_CaliYearMin-NN_TreeYearMin:NN_CaliYearMax-NN_TreeYearMin+1,:]\n",
    "            MLR_ReconClimVali = MLR_ReconClimTotal[NN_ValiYearMin-NN_TreeYearMin:NN_ValiYearMax-NN_TreeYearMin+1,:]\n",
    "            NN_ClimDataCali = NN_ClimDataToStat[NN_CaliYearMin-NN_YearMin:NN_CaliYearMax-NN_YearMin+1,:]\n",
    "            NN_ClimDataVali = NN_ClimDataToStat[NN_ValiYearMin-NN_YearMin:NN_ValiYearMax-NN_YearMin+1,:]\n",
    "        \n",
    "        # climate range\n",
    "        NN_ReconClimClimRange = NN_ReconClimTotal[NN_YearMin - NN_TreeYearMin:NN_YearMax - NN_TreeYearMin+1,:]\n",
    "        MLR_ReconClimClimRange = MLR_ReconClimTotal[NN_YearMin - NN_TreeYearMin:NN_YearMax - NN_TreeYearMin+1,:]\n",
    "\n",
    "            \n",
    "        NN_ClimDataClimRange = copy.deepcopy(NN_ClimDataToStat)\n",
    "        \n",
    "        # Matching the outputs to the distribution of the training. \n",
    "        if NN_DoDistributionMatch == 1:\n",
    "            \n",
    "            # changes final distribution of reconstruction to match the first two moments of the climate\n",
    "            # training. This could be a bit dangerous, fixing the variance damping. \n",
    "            \n",
    "            Clim_TrainMean = np.tile(np.mean(NN_ClimDataCali,axis = 0).reshape(1,-1),(np.shape(NN_ClimDataCali)[0],1))\n",
    "            Clim_TrainStdv = np.tile(np.std(NN_ClimDataCali,axis = 0).reshape(1,-1),(np.shape(NN_ClimDataCali)[0],1))\n",
    "            \n",
    "            NN_ReconClimCali = ChangeDist(NN_ReconClimCali,Clim_TrainMean,Clim_TrainStdv)\n",
    "            NN_ReconClimVali = ChangeDist(NN_ReconClimVali,Clim_TrainMean,Clim_TrainStdv)\n",
    "            NN_ReconClimClimRange = ChangeDist(NN_ReconClimClimRange,Clim_TrainMean,Clim_TrainStdv)\n",
    "            NN_ReconClimTotal = ChangeDist(NN_ReconClimTotal,Clim_TrainMean,Clim_TrainStdv)\n",
    "            \n",
    "            MLR_ReconClimCali = ChangeDist(MLR_ReconClimCali,Clim_TrainMean,Clim_TrainStdv)\n",
    "            MLR_ReconClimVali = ChangeDist(MLR_ReconClimVali,Clim_TrainMean,Clim_TrainStdv)\n",
    "            MLR_ReconClimClimRange = ChangeDist(MLR_ReconClimClimRange,Clim_TrainMean,Clim_TrainStdv)\n",
    "            MLR_ReconClimTotal = ChangeDist(MLR_ReconClimTotal,Clim_TrainMean,Clim_TrainStdv)\n",
    "            \n",
    "\n",
    "        print(\"%s seconds to run %s\" % (round(time.time() - SecStart,4),SecName))  \n",
    "        text = (\"%s minutes to run %s\" % (round((time.time() - SecStart)/60,4),SecName))\n",
    "        csv_status_file.append(text)\n",
    "        with open(txt_file_name, 'w') as f:\n",
    "            for item in csv_status_file:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # reconstruction statistics for NN and MLR\n",
    "        # average explained variance over the calibration period\n",
    "        NN_Rc2 =1-np.multiply(np.sum(np.multiply(NN_ClimDataCali - NN_ReconClimCali,NN_ClimDataCali - NN_ReconClimCali),axis=0),\n",
    "                               1./np.sum(np.multiply(NN_ClimDataCali-np.tile(np.mean(NN_ClimDataCali,axis=0),(np.shape(NN_ClimDataCali)[0],1)),\n",
    "                                                     NN_ClimDataCali-np.tile(np.mean(NN_ClimDataCali,axis=0),(np.shape(NN_ClimDataCali)[0],1))),\n",
    "                                         axis=0)).reshape(-1,1)\n",
    "        \n",
    "        # squared Pearson correlation over testing period\n",
    "        NN_Rv2=np.multiply(np.multiply(np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                               NN_ReconClimVali-np.tile(np.mean(NN_ReconClimVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),axis=0),\n",
    "                                       np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                               NN_ReconClimVali-np.tile(np.mean(NN_ReconClimVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),axis=0)),\n",
    "                            1./np.multiply(np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                                              NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),axis=0),\n",
    "                                           np.sum(np.multiply(NN_ReconClimVali-np.tile(np.mean(NN_ReconClimVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                                              NN_ReconClimVali-np.tile(np.mean(NN_ReconClimVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),axis=0)))\n",
    "        \n",
    "        # Reduction of error\n",
    "        NN_RE = 1-np.multiply(np.sum(np.multiply(NN_ClimDataVali - NN_ReconClimVali,NN_ClimDataVali - NN_ReconClimVali),axis=0),\n",
    "                               1./np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataCali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                                     NN_ClimDataVali-np.tile(np.mean(NN_ClimDataCali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),\n",
    "                                         axis=0)).reshape(-1,1)\n",
    "        \n",
    "        # Coefficient of efficiency\n",
    "        NN_CE = 1-np.multiply(np.sum(np.multiply(NN_ClimDataVali - NN_ReconClimVali,NN_ClimDataVali - NN_ReconClimVali),axis=0),\n",
    "                               1./np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                                     NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),\n",
    "                                         axis=0)).reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        # same for MLR\n",
    "        MLR_Rc2 =1-np.multiply(np.sum(np.multiply(NN_ClimDataCali - MLR_ReconClimCali,NN_ClimDataCali - MLR_ReconClimCali),axis=0),\n",
    "                               1./np.sum(np.multiply(NN_ClimDataCali-np.tile(np.mean(NN_ClimDataCali,axis=0),(np.shape(NN_ClimDataCali)[0],1)),\n",
    "                                                     NN_ClimDataCali-np.tile(np.mean(NN_ClimDataCali,axis=0),(np.shape(NN_ClimDataCali)[0],1))),\n",
    "                                         axis=0)).reshape(-1,1)\n",
    "        MLR_Rv2=np.multiply(np.multiply(np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                               MLR_ReconClimVali-np.tile(np.mean(MLR_ReconClimVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),axis=0),\n",
    "                                       np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                               MLR_ReconClimVali-np.tile(np.mean(MLR_ReconClimVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),axis=0)),\n",
    "                            1./np.multiply(np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                                              NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),axis=0),\n",
    "                                           np.sum(np.multiply(MLR_ReconClimVali-np.tile(np.mean(MLR_ReconClimVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                                              MLR_ReconClimVali-np.tile(np.mean(MLR_ReconClimVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),axis=0)))\n",
    "        MLR_RE = 1-np.multiply(np.sum(np.multiply(NN_ClimDataVali - MLR_ReconClimVali,NN_ClimDataVali - MLR_ReconClimVali),axis=0),\n",
    "                               1./np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataCali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                                     NN_ClimDataVali-np.tile(np.mean(NN_ClimDataCali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),\n",
    "                                         axis=0)).reshape(-1,1)\n",
    "        MLR_CE = 1-np.multiply(np.sum(np.multiply(NN_ClimDataVali - MLR_ReconClimVali,NN_ClimDataVali - MLR_ReconClimVali),axis=0),\n",
    "                               1./np.sum(np.multiply(NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1)),\n",
    "                                                     NN_ClimDataVali-np.tile(np.mean(NN_ClimDataVali,axis=0),(np.shape(NN_ClimDataVali)[0],1))),\n",
    "                                         axis=0)).reshape(-1,1)\n",
    "\n",
    "\n",
    "        # how many sites have skill accoring to CE in NN\n",
    "        print(np.sum(NN_CE>0)/np.sum(~np.isnan(NN_CE)))\n",
    "\n",
    "        CheckNumTrees = []\n",
    "        for i in range(len(NN_ClimTreesVal)):\n",
    "            CheckNumTrees.append(np.shape(NN_ClimTreesVal[i])[1])\n",
    "\n",
    "\n",
    "        # check the maximum number of trees in a site amongst all of them\n",
    "        InterMax = 0\n",
    "        InterMax2 = 0\n",
    "        for i in range(len(NN_ClimTreesVal)):\n",
    "            if np.shape(NN_ClimTreesVal[i])[1] > InterMax:\n",
    "                InterMax = np.shape(NN_ClimTreesVal[i])[1]\n",
    "            if len(NN_ReconClimTreesLatUsed[i]) > InterMax2:\n",
    "                InterMax2 = len(NN_ReconClimTreesLatUsed[i])\n",
    "    \n",
    "        # making an array of everything\n",
    "        NN_ClimTreesValArray = np.zeros((len(NN_ClimTreesVal),np.shape(NN_ClimTreesVal[0])[0],InterMax))\n",
    "        NN_ClimTreesValArray.fill(np.nan)\n",
    "\n",
    "        NN_ClimTreesLatArray = np.zeros((len(NN_ClimTreesVal),InterMax))\n",
    "        NN_ClimTreesLatArray.fill(np.nan)\n",
    "        NN_ClimTreesLonArray = np.zeros((len(NN_ClimTreesVal),InterMax))\n",
    "        NN_ClimTreesLonArray.fill(np.nan)\n",
    "        NN_ClimTreesIndArray = np.zeros((len(NN_ClimTreesVal),InterMax))\n",
    "        NN_ClimTreesIndArray.fill(np.nan)\n",
    "\n",
    "        # Sensitivity\n",
    "        NN_SensReconClimAllAdd_Sprt_Sens_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0],InterMax2))\n",
    "        NN_SensReconClimAllAdd_Sprt_x_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0],InterMax2))\n",
    "        NN_SensReconClimAllAdd_Sprt_y_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0]))\n",
    "        NN_SensReconClimAllAdd_Mean_Sens_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0],InterMax2))\n",
    "        NN_SensReconClimAllAdd_Mean_x_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0],InterMax2))\n",
    "        NN_SensReconClimAllAdd_Group_x_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0],InterMax2))\n",
    "        NN_SensReconClimAllAdd_Group_y_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0]))\n",
    "        NN_SensReconClimAllAdd_Group_Same_x_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0],InterMax2))\n",
    "        NN_SensReconClimAllAdd_Group_Same_y_Array = np.zeros((len(NN_ClimTreesVal),np.shape(x_to)[0]))\n",
    "        NN_ReconClimTreesLatUsedArray = np.zeros((len(NN_ClimTreesVal),InterMax2))\n",
    "        NN_ReconClimTreesLonUsedArray = np.zeros((len(NN_ClimTreesVal),InterMax2))\n",
    "        \n",
    "        NN_SensReconClimAllAdd_Sprt_Sens_Array.fill(np.nan)\n",
    "        NN_SensReconClimAllAdd_Sprt_x_Array.fill(np.nan)\n",
    "        NN_SensReconClimAllAdd_Sprt_y_Array.fill(np.nan)\n",
    "        NN_SensReconClimAllAdd_Mean_Sens_Array.fill(np.nan)\n",
    "        NN_SensReconClimAllAdd_Mean_x_Array.fill(np.nan)\n",
    "        NN_SensReconClimAllAdd_Group_x_Array.fill(np.nan)\n",
    "        NN_SensReconClimAllAdd_Group_y_Array.fill(np.nan)\n",
    "        NN_SensReconClimAllAdd_Group_Same_x_Array.fill(np.nan)\n",
    "        NN_SensReconClimAllAdd_Group_Same_y_Array.fill(np.nan)\n",
    "        NN_ReconClimTreesLatUsedArray.fill(np.nan)\n",
    "        NN_ReconClimTreesLonUsedArray.fill(np.nan)\n",
    "\n",
    "        #  filling it in. \n",
    "        for i in range(len(NN_ClimTreesVal)):\n",
    "            NN_ClimTreesValArray[i,:,0:np.shape(NN_ClimTreesVal[i])[1]] = NN_ClimTreesVal[i]\n",
    "            NN_ClimTreesLatArray[i,0:np.shape(NN_ClimTreesVal[i])[1]] = NN_ClimTreesLat[i]\n",
    "            NN_ClimTreesLonArray[i,0:np.shape(NN_ClimTreesVal[i])[1]] = NN_ClimTreesLon[i]\n",
    "            NN_ClimTreesIndArray[i,0:np.shape(NN_ClimTreesVal[i])[1]] = NN_ClimTreesInd[i]\n",
    "            \n",
    "            NN_SensReconClimAllAdd_Sprt_Sens_Array[i,:,0:len(NN_ReconClimTreesLatUsed[i])] = NN_SensReconClimAllAdd_Sprt_Sens[i]\n",
    "            NN_SensReconClimAllAdd_Sprt_x_Array[i,:,0:len(NN_ReconClimTreesLatUsed[i])] = NN_SensReconClimAllAdd_Sprt_x[i]\n",
    "            NN_SensReconClimAllAdd_Sprt_y_Array[i,:] = NN_SensReconClimAllAdd_Sprt_y[i].reshape(-1)\n",
    "            NN_SensReconClimAllAdd_Mean_Sens_Array[i,:,0:len(NN_ReconClimTreesLatUsed[i])] = NN_SensReconClimAllAdd_Mean_Sens[i]\n",
    "            NN_SensReconClimAllAdd_Mean_x_Array[i,:,0:len(NN_ReconClimTreesLatUsed[i])] = NN_SensReconClimAllAdd_Mean_x[i]\n",
    "            NN_SensReconClimAllAdd_Group_x_Array[i,:,0:len(NN_ReconClimTreesLatUsed[i])] = NN_SensReconClimAllAdd_Group_x[i]\n",
    "            NN_SensReconClimAllAdd_Group_y_Array[i,:] = NN_SensReconClimAllAdd_Group_y[i].reshape(-1)\n",
    "            NN_SensReconClimAllAdd_Group_Same_x_Array[i,:,0:len(NN_ReconClimTreesLatUsed[i])] = NN_SensReconClimAllAdd_Group_Same_x[i]\n",
    "            NN_SensReconClimAllAdd_Group_Same_y_Array[i,:] = NN_SensReconClimAllAdd_Group_Same_y[i].reshape(-1)\n",
    "        \n",
    "            NN_ReconClimTreesLatUsedArray[i,0:len(NN_ReconClimTreesLatUsed[i])] = NN_ReconClimTreesLatUsed[i]\n",
    "            NN_ReconClimTreesLonUsedArray[i,0:len(NN_ReconClimTreesLatUsed[i])] = NN_ReconClimTreesLonUsed[i]\n",
    "\n",
    "        # saving results\n",
    "        Results = type('', (), {})()\n",
    "\n",
    "        Results.NN_SeedNo = NN_SeedNo\n",
    "        \n",
    "        Results.NN_StandardizeBeforeMethod = NN_StandardizeBeforeMethod\n",
    "        Results.NN_DoDistributionMatch = NN_DoDistributionMatch\n",
    "        Results.NN_AllowDuplicateStands = NN_AllowDuplicateStands\n",
    "\n",
    "        Results.TreeFileSource = TreeFileSource\n",
    "        Results.NN_TreeFileSou = NN_TreeFileSou\n",
    "        Results.NN_CaliYearMin = NN_CaliYearMin\n",
    "        Results.NN_CaliYearMax = NN_CaliYearMax\n",
    "        Results.NN_ValiYearMin = NN_ValiYearMin\n",
    "        Results.NN_ValiYearMax = NN_ValiYearMax\n",
    "\n",
    "        Results.NN_TreeYearMin = NN_TreeYearMin\n",
    "        Results.NN_TreeYearMax = NN_TreeYearMax\n",
    "\n",
    "        Results.NN_LatMax = NN_LatMax\n",
    "        Results.NN_LatMin = NN_LatMin\n",
    "        Results.NN_LonMax = NN_LonMax\n",
    "        Results.NN_LonMin = NN_LonMin\n",
    "        #Results.NN_LatDeltaTree = NN_LatDeltaTree\n",
    "        #Results.NN_LonDeltaTree = NN_LonDeltaTree\n",
    "        Results.NN_LatDeltaTree = NN_LatDeltaTree\n",
    "        Results.NN_LonDeltaTree = NN_LatDeltaTree\n",
    "\n",
    "\n",
    "\n",
    "        Results.NN_ClimYearMin = NN_ClimYearMin\n",
    "        Results.NN_ClimYearMax = NN_ClimYearMax\n",
    "        Results.NN_ClimMonthMin = NN_ClimMonthMin\n",
    "        Results.NN_ClimMonthMax = NN_ClimMonthMax\n",
    "\n",
    "\n",
    "\n",
    "        # AR PROCESS\n",
    "        Results.NN_ARTimeDelta = NN_ARTimeDelta\n",
    "        Results.NN_ARPMin = NN_ARPMin\n",
    "        Results.NN_ARPMax = NN_ARPMax \n",
    "\n",
    "        Results.NN_YearMin = NN_YearMin\n",
    "        Results.NN_YearMax = NN_YearMax\n",
    "\n",
    "        # This choice is kind of arbitrary -- should have a way to check this\n",
    "        Results.NN_MinTrees = NN_MinTrees\n",
    "        Results.NN_MaxDistConstraint = NN_MaxDistConstraint\n",
    "\n",
    "        # This choice is arbitrary, keep Cook distance? Makes sense so as to not introduce data at different distances. \n",
    "        Results.NN_MaxDist = NN_MaxDist #km -- This is the Cook distance\n",
    "        Results.NN_ClimTreesMaxDistUsed = NN_ClimTreesMaxDistUsed\n",
    "\n",
    "        # NN PARAMS\n",
    "        Results.NN_HiddenLayers = NN_HiddenLayers\n",
    "        Results.NN_UseAdaptiveNumNodes  = NN_UseAdaptiveNumNodes\n",
    "        if NN_UseAdaptiveNumNodes != 0:\n",
    "            Results.NN_HiddenNodes = NN_HiddenNodesUsed\n",
    "        else:\n",
    "            Results.NN_HiddenNodes = NN_HiddenNodes\n",
    "        Results.NN_Activation = NN_Activation\n",
    "        Results.NN_Dropout = NN_Dropout\n",
    "        Results.NN_NNModels = NN_NNModels\n",
    "        Results.NN_UseEarlyStopping = NN_UseEarlyStopping\n",
    "        Results.NN_EarlyStoppingPatience = NN_EarlyStoppingPatience\n",
    "        Results.NN_TrainEpochs = NN_TrainEpochs\n",
    "        Results.NN_TestDropout = NN_TestDropout\n",
    "        Results.NN_DropoutMod = NN_DropoutMod\n",
    "        Results.NN_DropoutInput = NN_DropoutInput\n",
    "        Results.NN_TestRegWeight = NN_TestRegWeight\n",
    "        Results.NN_UseRandTrainTest = NN_UseRandTrainTest\n",
    "        Results.NN_DropoutUsed = NN_DropoutUsed\n",
    "        Results.NN_RegWeightUsed = NN_RegWeightUsed  \n",
    "        \n",
    "        # ORIGINAL DATA\n",
    "        Results.NN_ClimLimData = NN_ClimLimData\n",
    "        Results.NN_ClimLimLat = NN_ClimLimLat\n",
    "        Results.NN_ClimLimLon = NN_ClimLimLon\n",
    "        Results.NN_TreeLimData = NN_TreeLimData\n",
    "        Results.NN_TreeLimLon = NN_TreeLimLon\n",
    "        Results.NN_TreeLimLat = NN_TreeLimLat\n",
    "\n",
    "        # For AR and Standardizing Climate\n",
    "        Results.NN_ClimARCoef = NN_ClimARCoef\n",
    "        Results.NN_ClimARData = NN_ClimARData\n",
    "        Results.NN_ClimARCaliMean = NN_ClimARCaliMean\n",
    "        Results.NN_ClimARCaliStdev = NN_ClimARCaliStdev\n",
    "\n",
    "        # ORIGINAL RECON\n",
    "        Results.NN_ClimTreesValArray = NN_ClimTreesValArray\n",
    "        Results.NN_ClimTreesLatArray = NN_ClimTreesLatArray\n",
    "        Results.NN_ClimTreesLonArray = NN_ClimTreesLonArray\n",
    "        Results.NN_ClimTreesIndArray = NN_ClimTreesIndArray\n",
    "        if NN_SaveBasicResults == 0:\n",
    "            Results.NN_ReconClimARStdTotal = NN_ReconClimARStdTotal\n",
    "            Results.NN_ReconClimARStdCali = NN_ReconClimARStdCali\n",
    "            Results.NN_ReconClimARStdVali = NN_ReconClimARStdVali\n",
    "            Results.NN_ReconClimARStdClimRange = NN_ReconClimARStdClimRange\n",
    "            Results.MLR_ReconClimARStdTotal = MLR_ReconClimARTotalStd\n",
    "            Results.MLR_ReconClimARStdCali = MLR_ReconClimARCaliStd\n",
    "            Results.MLR_ReconClimARStdVali = MLR_ReconClimARValiStd\n",
    "            Results.MLR_ReconClimARStdClimRange = MLR_ReconClimARClimRangeStd\n",
    "            Results.NN_ClimARStd = NN_ClimARStd\n",
    "        Results.MLR_ReconTreesUsed = MLR_ReconTreesUsed\n",
    "        \n",
    "        \n",
    "        Results.NN_UseSameInputMLR = NN_UseSameInputMLR\n",
    "\n",
    "\n",
    "        # ADDING MEAN AND STDEV FROM CALI PERIOD\n",
    "        if NN_SaveBasicResults == 0:\n",
    "            Results.NN_ReconClimARTotal = NN_ReconClimARTotal\n",
    "            Results.NN_ReconClimARCali = NN_ReconClimARCali\n",
    "            Results.NN_ReconClimARVali = NN_ReconClimARVali\n",
    "            Results.NN_ReconClimARClimRange = NN_ReconClimARClimRange\n",
    "            Results.MLR_ReconClimARTotal = MLR_ReconClimARTotal\n",
    "            Results.MLR_ReconClimARCali = MLR_ReconClimARCali\n",
    "            Results.MLR_ReconClimARVali = MLR_ReconClimARVali\n",
    "            Results.MLR_ReconClimARClimRange = MLR_ReconClimARClimRange\n",
    "\n",
    "\n",
    "\n",
    "        # FINAL RECONSTRUCTION\n",
    "        Results.NN_ReconClimCali = NN_ReconClimCali\n",
    "        Results.NN_ReconClimVali = NN_ReconClimVali\n",
    "        Results.NN_ReconClimTotal = NN_ReconClimTotal\n",
    "        Results.NN_ReconClimClimRange = NN_ReconClimClimRange\n",
    "        Results.MLR_ReconClimCali = MLR_ReconClimCali\n",
    "        Results.MLR_ReconClimVali = MLR_ReconClimVali\n",
    "        Results.MLR_ReconClimTotal = MLR_ReconClimTotal\n",
    "        Results.MLR_ReconClimClimRange = MLR_ReconClimClimRange\n",
    "\n",
    "        # FOR STATS\n",
    "        Results.NN_ClimDataCali = NN_ClimDataCali\n",
    "        Results.NN_ClimDataClimRange = NN_ClimDataClimRange\n",
    "        Results.NN_ClimDataVali = NN_ClimDataVali\n",
    "\n",
    "        Results.Date = str(dt.datetime.now())\n",
    "\n",
    "\n",
    "        # STATS\n",
    "        Results.NN_Rc2 = NN_Rc2\n",
    "        Results.NN_Rv2 = NN_Rv2\n",
    "        Results.NN_RE = NN_RE\n",
    "        Results.NN_CE = NN_CE\n",
    "        Results.MLR_Rc2 = MLR_Rc2\n",
    "        Results.MLR_Rv2 = MLR_Rv2\n",
    "        Results.MLR_RE = MLR_RE\n",
    "        Results.MLR_CE = MLR_CE\n",
    "\n",
    "        Results.NN_Variable = NN_Variable\n",
    "\n",
    "\n",
    "        # SENSITIVITY\n",
    "        if NN_SaveBasicResults == 0:\n",
    "            Results.NN_SensReconClimAllAdd_Sprt_Sens_Array = NN_SensReconClimAllAdd_Sprt_Sens_Array\n",
    "            Results.NN_SensReconClimAllAdd_Sprt_x_Array = NN_SensReconClimAllAdd_Sprt_x_Array\n",
    "            Results.NN_SensReconClimAllAdd_Sprt_y_Array = NN_SensReconClimAllAdd_Sprt_y_Array\n",
    "            Results.NN_SensReconClimAllAdd_Mean_Sens_Array = NN_SensReconClimAllAdd_Mean_Sens_Array\n",
    "            Results.NN_SensReconClimAllAdd_Mean_x_Array = NN_SensReconClimAllAdd_Mean_x_Array\n",
    "            Results.NN_SensReconClimAllAdd_Group_x_Array = NN_SensReconClimAllAdd_Group_x_Array\n",
    "            Results.NN_SensReconClimAllAdd_Group_y_Array = NN_SensReconClimAllAdd_Group_y_Array\n",
    "            Results.NN_SensReconClimAllAdd_Group_Same_x_Array = NN_SensReconClimAllAdd_Group_Same_x_Array\n",
    "            Results.NN_SensReconClimAllAdd_Group_Same_y_Array = NN_SensReconClimAllAdd_Group_Same_y_Array\n",
    "            Results.NN_SensFactor = NN_SensFactor\n",
    "            \n",
    "        Results.NN_ReconClimTreesLatUsedArray = NN_ReconClimTreesLatUsedArray\n",
    "        Results.NN_ReconClimTreesLonUsedArray = NN_ReconClimTreesLonUsedArray\n",
    "\n",
    "        Results.NN_TrainEpochs = NN_TrainEpochs\n",
    "        Results.NN_Activation = NN_Activation\n",
    "        Results.NN_ActivationStatement = NN_ActivationStatement\n",
    "        Results.NN_RegType = NN_RegType\n",
    "        Results.NN_RegWeight = NN_RegWeight\n",
    "        Results.NN_DropoutInput = NN_DropoutInput\n",
    "        Results.NN_DropoutMod = NN_DropoutMod\n",
    "        Results.NN_HiddenNodesUsed = NN_HiddenNodesUsed\n",
    "\n",
    "        # TESTING CONFIGURATIONS\n",
    "\n",
    "        Results.NN_TestLayers = NN_TestLayers\n",
    "        Results.NN_TestRegWeight = NN_TestRegWeight\n",
    "        Results.NN_TestDropout = NN_TestDropout\n",
    "        Results.NN_TestRandTimes = NN_TestRandTimes\n",
    "        Results.NN_TestRandPropTrain = NN_TestRandPropTrain\n",
    "        \n",
    "        \n",
    "        Results.DownloadLocation = DownloadLocation\n",
    "\n",
    "        # actually storing them as a mat file\n",
    "        if NN_SaveResults == 1:\n",
    "            if NN_SaveResults == 1:# and np.nansum(NN_CE>0)>=0 and np.nansum(NN_Rv2>=0.27)>=0:\n",
    "             \n",
    "                scipy.io.savemat(SaveResultsFolder+'ResultsNNTreesSens_'+ #'/n/home06/atrevino/Research/Results/Python/NNTrees/\n",
    "                                 Results.NN_Variable+'_'+StartStat.strftime(\"%Y%m%d-%H%M\")[2:]+'_Run-'+str(OdysseyIndex)+\n",
    "                                 '_Instance-'+str(InterCountTestingAll)+'.mat',Results.__dict__)\n",
    "\n",
    "\n",
    "        \n",
    "        InterCountTestingAll = InterCountTestingAll + 1\n",
    "        print(\"%s seconds to run all\" % (round(time.time() - StartTime,4)))\n",
    "        text = (\"%s minutes to run all\" % (round((time.time() - StartTime)/60,4)))\n",
    "        csv_status_file.append(text)\n",
    "        with open(txt_file_name, 'w') as f:\n",
    "            for item in csv_status_file:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        print(\"Done with %s of %s Total: %s of %s RandTimes, %s of %s others\" %\n",
    "             (InterCountTestingAll, len(NN_TestRandTimes)*len(NN_TestRegWeight),\n",
    "              iTestRnd+1,len(NN_TestRandTimes), iTestReg+1, len(NN_TestRegWeight)))\n",
    "        text = (\"Done with %s of %s Total: %s of %s RandTimes, %s of %s others\" %\n",
    "             (InterCountTestingAll, len(NN_TestRandTimes)*len(NN_TestRegWeight),\n",
    "              iTestRnd+1,len(NN_TestRandTimes), iTestReg+1, len(NN_TestRegWeight)))\n",
    "        csv_status_file.append(text)\n",
    "        with open(txt_file_name, 'w') as f:\n",
    "            for item in csv_status_file:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        NN_TestRegUsed.append(NN_TestRegWeight[iTestReg])\n",
    "        NN_TestDrpUsed.append(NN_TestDropout[iTestReg])\n",
    "        NN_TestLayUsed.append(NN_TestLayers[iTestReg])\n",
    "        NN_TestRndUsed.append(NN_TestRandTimes[iTestRnd])\n",
    "        NN_TestPrpUsed.append(NN_TestRandPropTrain[iTestReg])\n",
    "        NN_TestDOFUsed.append(NN_TestDOFPropToUse[iTestReg])\n",
    "    \n",
    "        NN_TestSkillRc2.append(NN_Rc2)\n",
    "        NN_TestSkillRv2.append(NN_Rv2)\n",
    "        NN_TestSkillRE.append(NN_RE)\n",
    "        NN_TestSkillCE.append(NN_CE)\n",
    "        MLR_TestSkillRc2.append(MLR_Rc2)\n",
    "        MLR_TestSkillRv2.append(MLR_Rv2)\n",
    "        MLR_TestSkillRE.append(MLR_RE)\n",
    "        MLR_TestSkillCE.append(MLR_CE)\n",
    "        NN_TestDataSplit.append(Prelim_tr)\n",
    "        NN_TestReconClim.append(NN_ReconClimClimRange)\n",
    "        MLR_TestReconClim.append(MLR_ReconClimClimRange)\n",
    "        NN_TestClimClim.append(NN_ClimDataClimRange)\n",
    "        \n",
    "        text_results = ('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s'%\n",
    "               (str(NN_TestLayUsed[-1]),\n",
    "               str(NN_TestRegUsed[-1]),\n",
    "               str(NN_TestDrpUsed[-1]),\n",
    "               str(NN_TestPrpUsed[-1]),\n",
    "               str(NN_TestDOFUsed[-1]),\n",
    "               str(NN_TestRndUsed[-1]),\n",
    "               str(np.round(np.sum((NN_CE-MLR_CE)>0)/np.sum(~np.isnan(NN_CE-MLR_CE)),4)),\n",
    "               str(np.round(np.sum((NN_Rv2-MLR_Rv2)>0)/np.sum(~np.isnan(NN_Rv2-MLR_Rv2)),4)),\n",
    "               str(np.round(np.sum((NN_RE-MLR_RE)>0)/np.sum(~np.isnan(NN_RE-MLR_RE)),4)),\n",
    "               str(np.round(np.sum((NN_Rv2-MLR_Rc2)>0)/np.sum(~np.isnan(NN_Rv2-MLR_Rc2)),4)),\n",
    "               str(np.round(np.sum(NN_CE>0)/np.sum(~np.isnan(NN_CE)),4)),\n",
    "               str(np.round(np.sum(MLR_CE>0)/np.sum(~np.isnan(MLR_CE)),4)),\n",
    "               str(np.round(np.nanmean(NN_CE),4)),\n",
    "               str(np.round(np.nanmean(NN_RE),4)),\n",
    "               str(np.round(np.nanmean(NN_Rv2),4)),\n",
    "               str(np.round(np.nanmean(NN_Rv2),4)),\n",
    "               str(np.round(np.nanmean(MLR_CE),4)),\n",
    "               str(np.round(np.nanmean(MLR_RE),4)),\n",
    "               str(np.round(np.nanmean(MLR_Rv2),4)),\n",
    "               str(np.round(np.nanmean(MLR_Rv2),4))))\n",
    "        csv_results_file.append(text_results)\n",
    "        with open(results_file_name, 'w') as g:\n",
    "            for item in csv_results_file:\n",
    "                g.write(\"%s\\n\" % item)\n",
    "                \n",
    "\n",
    "if TestingCode == 0: \n",
    "    csv_status_file[0] = 'DONE ALL TESTS'\n",
    "\n",
    "with open(txt_file_name, 'w') as f:\n",
    "    for item in csv_status_file:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "        \n",
    "# DONE. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_root]",
   "language": "python",
   "name": "conda-env-my_root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
